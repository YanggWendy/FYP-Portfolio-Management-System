{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO8e/kSsk281NOcMdufyRPq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "df137d18c19945d29b8e6dcbb690c366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fde3028a59449bd8d5a4bafa6221f2c",
              "IPY_MODEL_c7b894306c06446381cb949751c18c20",
              "IPY_MODEL_a49a81ad051b4d7aab7ac67c5539968e"
            ],
            "layout": "IPY_MODEL_6c35dadbd47b44338f5a2d4fc3842c90"
          }
        },
        "4fde3028a59449bd8d5a4bafa6221f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca02c0b7e75c468da13d64e74cc83fb3",
            "placeholder": "​",
            "style": "IPY_MODEL_af69eb75294d4ddf908287ee58ad9c2f",
            "value": "100%"
          }
        },
        "c7b894306c06446381cb949751c18c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a211d5ca2fc4477186d9319a83ec31fa",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59df5c89bd24497a862f5100f2492308",
            "value": 500
          }
        },
        "a49a81ad051b4d7aab7ac67c5539968e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6af157aca6e4521ade9d2cf05854f03",
            "placeholder": "​",
            "style": "IPY_MODEL_e220997d63bf49498b499dd31ac0bdac",
            "value": " 500/500 [00:01&lt;00:00, 585.34it/s]"
          }
        },
        "6c35dadbd47b44338f5a2d4fc3842c90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca02c0b7e75c468da13d64e74cc83fb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af69eb75294d4ddf908287ee58ad9c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a211d5ca2fc4477186d9319a83ec31fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59df5c89bd24497a862f5100f2492308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6af157aca6e4521ade9d2cf05854f03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e220997d63bf49498b499dd31ac0bdac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32203cbb24a64951996d07990691e92e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5fca29a65f914059b798fed69c2f66c1",
              "IPY_MODEL_8e5e99afab8241c3ac83fc93f06f7483",
              "IPY_MODEL_8017916114474b07bbf8d7fe723f3670"
            ],
            "layout": "IPY_MODEL_17dd5ff635cd48b984969ba792d6fd57"
          }
        },
        "5fca29a65f914059b798fed69c2f66c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78acf403d4934bca8a92bd04bcd97155",
            "placeholder": "​",
            "style": "IPY_MODEL_7fa53c6d694f4ef293e1ed638a068da6",
            "value": "100%"
          }
        },
        "8e5e99afab8241c3ac83fc93f06f7483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_877aa45ee28c421cac99cd15c1571e28",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ae562f559e34615b74407017b83b3f5",
            "value": 26
          }
        },
        "8017916114474b07bbf8d7fe723f3670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_912e9d30a0e14488a990259d500f060c",
            "placeholder": "​",
            "style": "IPY_MODEL_1a93342296de4696950d4574dd88b613",
            "value": " 26/26 [05:38&lt;00:00, 12.30s/it]"
          }
        },
        "17dd5ff635cd48b984969ba792d6fd57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78acf403d4934bca8a92bd04bcd97155": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fa53c6d694f4ef293e1ed638a068da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "877aa45ee28c421cac99cd15c1571e28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ae562f559e34615b74407017b83b3f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "912e9d30a0e14488a990259d500f060c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a93342296de4696950d4574dd88b613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b19673bd7f7a43cca8c8480ff9d72dc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db66ec605bf14d1aabf48b457425a9c7",
              "IPY_MODEL_0148daf828a141539ac9b6f5bdd0869e",
              "IPY_MODEL_e4e93352d4534a35b9294cb136f27dee"
            ],
            "layout": "IPY_MODEL_48d1e82850a740bc8098e1f1dfbd5e93"
          }
        },
        "db66ec605bf14d1aabf48b457425a9c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baf6cecf30db49e3ab4fb6e3a439d682",
            "placeholder": "​",
            "style": "IPY_MODEL_125692f83a414fe8be9b8e2bfec81f21",
            "value": "100%"
          }
        },
        "0148daf828a141539ac9b6f5bdd0869e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27c56b8b751c4da4aeafeb947a605e77",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b4fa05dbacc4459be3ece8c3541f402",
            "value": 26
          }
        },
        "e4e93352d4534a35b9294cb136f27dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbe358eb35cd482a98a626041fef8ebb",
            "placeholder": "​",
            "style": "IPY_MODEL_45564c76f9864c52b1e233d1c9783f18",
            "value": " 26/26 [04:37&lt;00:00, 10.40s/it]"
          }
        },
        "48d1e82850a740bc8098e1f1dfbd5e93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baf6cecf30db49e3ab4fb6e3a439d682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "125692f83a414fe8be9b8e2bfec81f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27c56b8b751c4da4aeafeb947a605e77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b4fa05dbacc4459be3ece8c3541f402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dbe358eb35cd482a98a626041fef8ebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45564c76f9864c52b1e233d1c9783f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10616f8434b94d13bab1807481625117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8a93a35bbf44834a56aa0dac6e77097",
              "IPY_MODEL_2d884b6b89934f0696536297f17c2cef",
              "IPY_MODEL_45de9013b68647988f70f78833b96e90"
            ],
            "layout": "IPY_MODEL_ca2cb7a16e5c455aaef2d7c49683a03e"
          }
        },
        "b8a93a35bbf44834a56aa0dac6e77097": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_157cb1810f694208bcfaa14f5228ddea",
            "placeholder": "​",
            "style": "IPY_MODEL_b5d73444c3f04f6589677dc5595bec52",
            "value": "100%"
          }
        },
        "2d884b6b89934f0696536297f17c2cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bd95a16eefe45d3beb86601a403bff4",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4a5ecbde6074d36b358907b01befdb6",
            "value": 26
          }
        },
        "45de9013b68647988f70f78833b96e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4b6e8c3ac2e46499966fdd253687ca4",
            "placeholder": "​",
            "style": "IPY_MODEL_b17d573f93a14564abe4f280a50eb892",
            "value": " 26/26 [04:08&lt;00:00,  9.04s/it]"
          }
        },
        "ca2cb7a16e5c455aaef2d7c49683a03e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "157cb1810f694208bcfaa14f5228ddea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5d73444c3f04f6589677dc5595bec52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bd95a16eefe45d3beb86601a403bff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4a5ecbde6074d36b358907b01befdb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4b6e8c3ac2e46499966fdd253687ca4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b17d573f93a14564abe4f280a50eb892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YanggWendy/FYP-Portfolio-Management-System/blob/main/FYP_2000_2022_123.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP9dxNHlBc2H",
        "outputId": "f3c9dd25-9d8b-4609-e308-13e49b74e66a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "FOLDERNAME = 'FYP Data/'\n",
        "assert FOLDERNAME is not None, \"[1]Enter the foldername.\"\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance\n",
        "!pip install fundamentalanalysis\n",
        "PATH = '/content/drive/My Drive/FYP Data/' "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzRRLz-MBoRr",
        "outputId": "640d6394-49d2-4142-f4f8-73b938817705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.9/dist-packages (0.2.17)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (4.11.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2.3.7)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2.27.1)\n",
            "Requirement already satisfied: cryptography>=3.3.2 in /usr/local/lib/python3.9/dist-packages (from yfinance) (40.0.1)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.9/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2022.7.1)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.5.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (4.9.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=3.3.2->yfinance) (1.15.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.9/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (2022.12.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fundamentalanalysis\n",
            "  Downloading fundamentalanalysis-0.2.14-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: fundamentalanalysis\n",
            "Successfully installed fundamentalanalysis-0.2.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import statistics\n",
        "import fundamentalanalysis as fa\n",
        "import datetime\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau,TensorBoard\n",
        "from tensorflow.keras.models import load_model\n",
        "import math\n",
        "from keras import Input # for instantiating a keras tensor\n",
        "from keras.layers import Bidirectional, GRU, RepeatVector, Dense, TimeDistributed # for creating layers inside the Neural Network\n",
        "from keras.optimizers import SGD"
      ],
      "metadata": {
        "id": "fT-42N8SBrwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp500url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
        "data_table = pd.read_html(sp500url)\n",
        "data_table[0]\n",
        "SP500_ticker = data_table[0]['Symbol'].tolist()\n",
        "\n",
        "#remove invalid stock ticker\n",
        "SP500_ticker.remove('BRK.B')\n",
        "SP500_ticker.remove('BF.B')\n",
        "SP500_ticker.remove('GEHC')\n",
        "#SP500_ticker.remove('BG')\n",
        "#SP500_ticker.remove('FICO')\n",
        "#SP500_ticker.remove('PODD')"
      ],
      "metadata": {
        "id": "i-a9SCbzBtXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Technical_list = ['Close','Volume','VEMA12','VSTD20','V20','AR','BR','AU','AD']\n",
        "fundamental_list = ['marketCap', 'peRatio', 'pbRatio', 'evToSales','receivablesTurnover', 'payablesTurnover','debtToAssets', 'inventoryTurnover',\n",
        " 'roe','revenuePerShare','cashPerShare']"
      ],
      "metadata": {
        "id": "YOEh1FF-BuzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load data"
      ],
      "metadata": {
        "id": "CmJs-VWnIwNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data = pd.read_csv((\"drive/My Drive/FYP Data/price.csv\"),index_col = [0], header = [0,1])\n",
        "stock_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "shQt06dqB6z0",
        "outputId": "97147162-2f8b-412a-af70-4492aa5f646d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  ALGN                                                  \\\n",
              "                  Open        High         Low       Close   Adj Close   \n",
              "Date                                                                     \n",
              "2023-01-31  265.260010  269.779999  265.000000  269.730011  269.730011   \n",
              "2023-01-30  263.500000  268.549988  261.500000  265.820007  265.820007   \n",
              "2023-01-27  260.000000  270.000000  259.019989  269.200012  269.200012   \n",
              "2023-01-26  255.860001  263.100006  254.990005  262.950012  262.950012   \n",
              "2023-01-25  246.820007  252.470001  245.639999  252.080002  252.080002   \n",
              "...                ...         ...         ...         ...         ...   \n",
              "1993-02-04         NaN         NaN         NaN         NaN         NaN   \n",
              "1993-02-03         NaN         NaN         NaN         NaN         NaN   \n",
              "1993-02-02         NaN         NaN         NaN         NaN         NaN   \n",
              "1993-02-01         NaN         NaN         NaN         NaN         NaN   \n",
              "1993-01-29         NaN         NaN         NaN         NaN         NaN   \n",
              "\n",
              "                             SBAC                                      ...  \\\n",
              "               Volume        Open        High         Low       Close  ...   \n",
              "Date                                                                   ...   \n",
              "2023-01-31   844700.0  291.950012  297.730011  289.690002  297.529999  ...   \n",
              "2023-01-30  1454300.0  288.920013  295.019989  287.100006  290.200012  ...   \n",
              "2023-01-27  1165100.0  289.190002  291.970001  286.890015  289.029999  ...   \n",
              "2023-01-26  1177000.0  292.429993  294.559998  288.970001  291.079987  ...   \n",
              "2023-01-25   820700.0  289.140015  292.380005  286.160004  291.649994  ...   \n",
              "...               ...         ...         ...         ...         ...  ...   \n",
              "1993-02-04        NaN         NaN         NaN         NaN         NaN  ...   \n",
              "1993-02-03        NaN         NaN         NaN         NaN         NaN  ...   \n",
              "1993-02-02        NaN         NaN         NaN         NaN         NaN  ...   \n",
              "1993-02-01        NaN         NaN         NaN         NaN         NaN  ...   \n",
              "1993-01-29        NaN         NaN         NaN         NaN         NaN  ...   \n",
              "\n",
              "                   XOM                                           DD  \\\n",
              "                   Low       Close   Adj Close    Volume       Open   \n",
              "Date                                                                  \n",
              "2023-01-31  110.430000  116.010002  115.124138  27861800  72.910004   \n",
              "2023-01-30  113.150002  113.559998  112.692841  18672100  73.099998   \n",
              "2023-01-27  115.389999  115.610001  114.727188  15179200  73.059998   \n",
              "2023-01-26  114.339996  117.760002  116.860771  21019700  73.169998   \n",
              "2023-01-25  111.389999  113.209999  112.345512  11914700  73.790001   \n",
              "...                ...         ...         ...       ...        ...   \n",
              "1993-02-04   15.375000   15.500000    5.945010   4631600  25.559553   \n",
              "1993-02-03   15.406250   15.625000    5.923919   5699600  25.737461   \n",
              "1993-02-02   15.343750   15.562500    5.900226   3359600  25.263039   \n",
              "1993-02-01   15.281250   15.375000    5.829138   3186400  25.322342   \n",
              "1993-01-29   15.156250   15.281250    5.793594   4369600  24.670010   \n",
              "\n",
              "                                                                   \n",
              "                 High        Low      Close  Adj Close     Volume  \n",
              "Date                                                               \n",
              "2023-01-31  74.000000  72.269997  73.949997  73.584908  2736000.0  \n",
              "2023-01-30  73.559998  72.449997  72.650002  72.291336  2542000.0  \n",
              "2023-01-27  74.190002  73.059998  73.550003  73.186890  2171300.0  \n",
              "2023-01-26  73.790001  72.099998  73.730003  73.366005  3134500.0  \n",
              "2023-01-25  74.180000  72.980003  73.529999  73.166985  3193100.0  \n",
              "...               ...        ...        ...        ...        ...  \n",
              "1993-02-04  26.211885  25.559553  26.033976   9.430413  1581710.0  \n",
              "1993-02-03  25.915371  25.381643  25.500250   9.237073  1414770.0  \n",
              "1993-02-02  25.737461  25.263039  25.678158   9.301520   895825.0  \n",
              "1993-02-01  25.440947  25.085129  25.322342   9.172627   943461.0  \n",
              "1993-01-29  25.263039  24.551403  25.203735   9.129669  1475054.0  \n",
              "\n",
              "[7556 rows x 3000 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f8cf7c4-2982-4442-8163-295584c52da0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"6\" halign=\"left\">ALGN</th>\n",
              "      <th colspan=\"4\" halign=\"left\">SBAC</th>\n",
              "      <th>...</th>\n",
              "      <th colspan=\"4\" halign=\"left\">XOM</th>\n",
              "      <th colspan=\"6\" halign=\"left\">DD</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>...</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2023-01-31</th>\n",
              "      <td>265.260010</td>\n",
              "      <td>269.779999</td>\n",
              "      <td>265.000000</td>\n",
              "      <td>269.730011</td>\n",
              "      <td>269.730011</td>\n",
              "      <td>844700.0</td>\n",
              "      <td>291.950012</td>\n",
              "      <td>297.730011</td>\n",
              "      <td>289.690002</td>\n",
              "      <td>297.529999</td>\n",
              "      <td>...</td>\n",
              "      <td>110.430000</td>\n",
              "      <td>116.010002</td>\n",
              "      <td>115.124138</td>\n",
              "      <td>27861800</td>\n",
              "      <td>72.910004</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>72.269997</td>\n",
              "      <td>73.949997</td>\n",
              "      <td>73.584908</td>\n",
              "      <td>2736000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-30</th>\n",
              "      <td>263.500000</td>\n",
              "      <td>268.549988</td>\n",
              "      <td>261.500000</td>\n",
              "      <td>265.820007</td>\n",
              "      <td>265.820007</td>\n",
              "      <td>1454300.0</td>\n",
              "      <td>288.920013</td>\n",
              "      <td>295.019989</td>\n",
              "      <td>287.100006</td>\n",
              "      <td>290.200012</td>\n",
              "      <td>...</td>\n",
              "      <td>113.150002</td>\n",
              "      <td>113.559998</td>\n",
              "      <td>112.692841</td>\n",
              "      <td>18672100</td>\n",
              "      <td>73.099998</td>\n",
              "      <td>73.559998</td>\n",
              "      <td>72.449997</td>\n",
              "      <td>72.650002</td>\n",
              "      <td>72.291336</td>\n",
              "      <td>2542000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-27</th>\n",
              "      <td>260.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>259.019989</td>\n",
              "      <td>269.200012</td>\n",
              "      <td>269.200012</td>\n",
              "      <td>1165100.0</td>\n",
              "      <td>289.190002</td>\n",
              "      <td>291.970001</td>\n",
              "      <td>286.890015</td>\n",
              "      <td>289.029999</td>\n",
              "      <td>...</td>\n",
              "      <td>115.389999</td>\n",
              "      <td>115.610001</td>\n",
              "      <td>114.727188</td>\n",
              "      <td>15179200</td>\n",
              "      <td>73.059998</td>\n",
              "      <td>74.190002</td>\n",
              "      <td>73.059998</td>\n",
              "      <td>73.550003</td>\n",
              "      <td>73.186890</td>\n",
              "      <td>2171300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-26</th>\n",
              "      <td>255.860001</td>\n",
              "      <td>263.100006</td>\n",
              "      <td>254.990005</td>\n",
              "      <td>262.950012</td>\n",
              "      <td>262.950012</td>\n",
              "      <td>1177000.0</td>\n",
              "      <td>292.429993</td>\n",
              "      <td>294.559998</td>\n",
              "      <td>288.970001</td>\n",
              "      <td>291.079987</td>\n",
              "      <td>...</td>\n",
              "      <td>114.339996</td>\n",
              "      <td>117.760002</td>\n",
              "      <td>116.860771</td>\n",
              "      <td>21019700</td>\n",
              "      <td>73.169998</td>\n",
              "      <td>73.790001</td>\n",
              "      <td>72.099998</td>\n",
              "      <td>73.730003</td>\n",
              "      <td>73.366005</td>\n",
              "      <td>3134500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-25</th>\n",
              "      <td>246.820007</td>\n",
              "      <td>252.470001</td>\n",
              "      <td>245.639999</td>\n",
              "      <td>252.080002</td>\n",
              "      <td>252.080002</td>\n",
              "      <td>820700.0</td>\n",
              "      <td>289.140015</td>\n",
              "      <td>292.380005</td>\n",
              "      <td>286.160004</td>\n",
              "      <td>291.649994</td>\n",
              "      <td>...</td>\n",
              "      <td>111.389999</td>\n",
              "      <td>113.209999</td>\n",
              "      <td>112.345512</td>\n",
              "      <td>11914700</td>\n",
              "      <td>73.790001</td>\n",
              "      <td>74.180000</td>\n",
              "      <td>72.980003</td>\n",
              "      <td>73.529999</td>\n",
              "      <td>73.166985</td>\n",
              "      <td>3193100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1993-02-04</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>15.375000</td>\n",
              "      <td>15.500000</td>\n",
              "      <td>5.945010</td>\n",
              "      <td>4631600</td>\n",
              "      <td>25.559553</td>\n",
              "      <td>26.211885</td>\n",
              "      <td>25.559553</td>\n",
              "      <td>26.033976</td>\n",
              "      <td>9.430413</td>\n",
              "      <td>1581710.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1993-02-03</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>15.406250</td>\n",
              "      <td>15.625000</td>\n",
              "      <td>5.923919</td>\n",
              "      <td>5699600</td>\n",
              "      <td>25.737461</td>\n",
              "      <td>25.915371</td>\n",
              "      <td>25.381643</td>\n",
              "      <td>25.500250</td>\n",
              "      <td>9.237073</td>\n",
              "      <td>1414770.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1993-02-02</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>15.343750</td>\n",
              "      <td>15.562500</td>\n",
              "      <td>5.900226</td>\n",
              "      <td>3359600</td>\n",
              "      <td>25.263039</td>\n",
              "      <td>25.737461</td>\n",
              "      <td>25.263039</td>\n",
              "      <td>25.678158</td>\n",
              "      <td>9.301520</td>\n",
              "      <td>895825.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1993-02-01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>15.281250</td>\n",
              "      <td>15.375000</td>\n",
              "      <td>5.829138</td>\n",
              "      <td>3186400</td>\n",
              "      <td>25.322342</td>\n",
              "      <td>25.440947</td>\n",
              "      <td>25.085129</td>\n",
              "      <td>25.322342</td>\n",
              "      <td>9.172627</td>\n",
              "      <td>943461.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1993-01-29</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>15.156250</td>\n",
              "      <td>15.281250</td>\n",
              "      <td>5.793594</td>\n",
              "      <td>4369600</td>\n",
              "      <td>24.670010</td>\n",
              "      <td>25.263039</td>\n",
              "      <td>24.551403</td>\n",
              "      <td>25.203735</td>\n",
              "      <td>9.129669</td>\n",
              "      <td>1475054.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7556 rows × 3000 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f8cf7c4-2982-4442-8163-295584c52da0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9f8cf7c4-2982-4442-8163-295584c52da0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9f8cf7c4-2982-4442-8163-295584c52da0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_df = pd.read_csv(\"drive/My Drive/FYP Data/label.csv\",index_col = 0)\n",
        "label_df = label_df.reindex(index=label_df.index[::-1])\n",
        "label_df.reset_index()\n",
        "label_df = label_df[84:]\n",
        "label_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "6-c7m8olBkSF",
        "outputId": "f2294c6a-81b8-4f51-abf0-a6d3b4142e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            MMM  AOS  ABT  ABBV  ACN  ATVI  ADM  ADBE  ADP  AAP  ...  WTW  \\\n",
              "Date                                                             ...        \n",
              "2000-01-31    0    0    1     0    0     0    0     1    0    0  ...    0   \n",
              "2000-02-29    0    0    0     0    0     0    0     0    0    0  ...    0   \n",
              "2000-03-31    0    1    1     0    0     0    0     1    1    0  ...    0   \n",
              "2000-04-28    0    0    1     0    0     0    1     0    0    0  ...    0   \n",
              "2000-05-31    0    0    1     0    0     1    0     1    0    0  ...    0   \n",
              "...         ...  ...  ...   ...  ...   ...  ...   ...  ...  ...  ...  ...   \n",
              "2022-08-31    0    0    1     1    0     1    0     0    0    0  ...    1   \n",
              "2022-09-30    0    0    0     0    0     0    1     0    0    1  ...    0   \n",
              "2022-10-31    0    1    0     0    0     0    0     0    0    0  ...    1   \n",
              "2022-11-30    0    0    1     1    0     1    0     0    0    0  ...    1   \n",
              "2022-12-30    0    1    0     0    0     0    0     0    0    0  ...    0   \n",
              "\n",
              "            GWW  WYNN  XEL  XYL  YUM  ZBRA  ZBH  ZION  ZTS  \n",
              "Date                                                        \n",
              "2000-01-31    0     0    0    0    0     1    0     0    0  \n",
              "2000-02-29    1     0    0    0    1     0    0     0    0  \n",
              "2000-03-31    0     0    1    0    1     1    0     0    0  \n",
              "2000-04-28    0     0    0    0    0     0    0     1    0  \n",
              "2000-05-31    0     0    0    0    0     0    0     0    0  \n",
              "...         ...   ...  ...  ...  ...   ...  ...   ...  ...  \n",
              "2022-08-31    0     1    0    1    1     0    1     0    1  \n",
              "2022-09-30    1     0    0    1    0     0    0     0    0  \n",
              "2022-10-31    0     1    0    0    0     0    0     0    0  \n",
              "2022-11-30    0     1    1    0    1     0    1     0    0  \n",
              "2022-12-30    0     1    0    0    0     1    0     0    1  \n",
              "\n",
              "[276 rows x 500 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cdefe14e-2f2c-4d06-8eac-5ebc77116ceb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MMM</th>\n",
              "      <th>AOS</th>\n",
              "      <th>ABT</th>\n",
              "      <th>ABBV</th>\n",
              "      <th>ACN</th>\n",
              "      <th>ATVI</th>\n",
              "      <th>ADM</th>\n",
              "      <th>ADBE</th>\n",
              "      <th>ADP</th>\n",
              "      <th>AAP</th>\n",
              "      <th>...</th>\n",
              "      <th>WTW</th>\n",
              "      <th>GWW</th>\n",
              "      <th>WYNN</th>\n",
              "      <th>XEL</th>\n",
              "      <th>XYL</th>\n",
              "      <th>YUM</th>\n",
              "      <th>ZBRA</th>\n",
              "      <th>ZBH</th>\n",
              "      <th>ZION</th>\n",
              "      <th>ZTS</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2000-01-31</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-02-29</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-03-31</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-04-28</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-05-31</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-08-31</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-09-30</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-10-31</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-11-30</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-30</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>276 rows × 500 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cdefe14e-2f2c-4d06-8eac-5ebc77116ceb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cdefe14e-2f2c-4d06-8eac-5ebc77116ceb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cdefe14e-2f2c-4d06-8eac-5ebc77116ceb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read data\n",
        "multi_factor_data = pd.read_csv(\"drive/My Drive/FYP Data/multi_factor_data_improved.csv\",index_col = 0,header = [0,1])\n",
        "multi_factor_data[np.isnan(multi_factor_data)] = 0\n",
        "multi_factor_data = multi_factor_data[84:]\n",
        "#get time list\n",
        "time = multi_factor_data.index.values\n",
        "time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFtIY96ECcwc",
        "outputId": "b25c984b-e7c0-4d35-f012-8c72a4fbba50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['2000-01-31', '2000-02-29', '2000-03-31', '2000-04-28',\n",
              "       '2000-05-31', '2000-06-30', '2000-07-31', '2000-08-31',\n",
              "       '2000-09-29', '2000-10-31', '2000-11-30', '2000-12-29',\n",
              "       '2001-01-31', '2001-02-28', '2001-03-30', '2001-04-30',\n",
              "       '2001-05-31', '2001-06-29', '2001-07-31', '2001-08-31',\n",
              "       '2001-09-28', '2001-10-31', '2001-11-30', '2001-12-31',\n",
              "       '2002-01-31', '2002-02-28', '2002-03-28', '2002-04-30',\n",
              "       '2002-05-31', '2002-06-28', '2002-07-31', '2002-08-30',\n",
              "       '2002-09-30', '2002-10-31', '2002-11-29', '2002-12-31',\n",
              "       '2003-01-31', '2003-02-28', '2003-03-31', '2003-04-30',\n",
              "       '2003-05-30', '2003-06-30', '2003-07-31', '2003-08-29',\n",
              "       '2003-09-30', '2003-10-31', '2003-11-28', '2003-12-31',\n",
              "       '2004-01-30', '2004-02-27', '2004-03-31', '2004-04-30',\n",
              "       '2004-05-28', '2004-06-30', '2004-07-30', '2004-08-31',\n",
              "       '2004-09-30', '2004-10-29', '2004-11-30', '2004-12-31',\n",
              "       '2005-01-31', '2005-02-28', '2005-03-31', '2005-04-29',\n",
              "       '2005-05-31', '2005-06-30', '2005-07-29', '2005-08-31',\n",
              "       '2005-09-30', '2005-10-31', '2005-11-30', '2005-12-30',\n",
              "       '2006-01-31', '2006-02-28', '2006-03-31', '2006-04-28',\n",
              "       '2006-05-31', '2006-06-30', '2006-07-31', '2006-08-31',\n",
              "       '2006-09-29', '2006-10-31', '2006-11-30', '2006-12-29',\n",
              "       '2007-01-31', '2007-02-28', '2007-03-30', '2007-04-30',\n",
              "       '2007-05-31', '2007-06-29', '2007-07-31', '2007-08-31',\n",
              "       '2007-09-28', '2007-10-31', '2007-11-30', '2007-12-31',\n",
              "       '2008-01-31', '2008-02-29', '2008-03-31', '2008-04-30',\n",
              "       '2008-05-30', '2008-06-30', '2008-07-31', '2008-08-29',\n",
              "       '2008-09-30', '2008-10-31', '2008-11-28', '2008-12-31',\n",
              "       '2009-01-30', '2009-02-27', '2009-03-31', '2009-04-30',\n",
              "       '2009-05-29', '2009-06-30', '2009-07-31', '2009-08-31',\n",
              "       '2009-09-30', '2009-10-30', '2009-11-30', '2009-12-31',\n",
              "       '2010-01-29', '2010-02-26', '2010-03-31', '2010-04-30',\n",
              "       '2010-05-28', '2010-06-30', '2010-07-30', '2010-08-31',\n",
              "       '2010-09-30', '2010-10-29', '2010-11-30', '2010-12-31',\n",
              "       '2011-01-31', '2011-02-28', '2011-03-31', '2011-04-29',\n",
              "       '2011-05-31', '2011-06-30', '2011-07-29', '2011-08-31',\n",
              "       '2011-09-30', '2011-10-31', '2011-11-30', '2011-12-30',\n",
              "       '2012-01-31', '2012-02-29', '2012-03-30', '2012-04-30',\n",
              "       '2012-05-31', '2012-06-29', '2012-07-31', '2012-08-31',\n",
              "       '2012-09-28', '2012-10-31', '2012-11-30', '2012-12-31',\n",
              "       '2013-01-31', '2013-02-28', '2013-03-28', '2013-04-30',\n",
              "       '2013-05-31', '2013-06-28', '2013-07-31', '2013-08-30',\n",
              "       '2013-09-30', '2013-10-31', '2013-11-29', '2013-12-31',\n",
              "       '2014-01-31', '2014-02-28', '2014-03-31', '2014-04-30',\n",
              "       '2014-05-30', '2014-06-30', '2014-07-31', '2014-08-29',\n",
              "       '2014-09-30', '2014-10-31', '2014-11-28', '2014-12-31',\n",
              "       '2015-01-30', '2015-02-27', '2015-03-31', '2015-04-30',\n",
              "       '2015-05-29', '2015-06-30', '2015-07-31', '2015-08-31',\n",
              "       '2015-09-30', '2015-10-30', '2015-11-30', '2015-12-31',\n",
              "       '2016-01-29', '2016-02-29', '2016-03-31', '2016-04-29',\n",
              "       '2016-05-31', '2016-06-30', '2016-07-29', '2016-08-31',\n",
              "       '2016-09-30', '2016-10-31', '2016-11-30', '2016-12-30',\n",
              "       '2017-01-31', '2017-02-28', '2017-03-31', '2017-04-28',\n",
              "       '2017-05-31', '2017-06-30', '2017-07-31', '2017-08-31',\n",
              "       '2017-09-29', '2017-10-31', '2017-11-30', '2017-12-29',\n",
              "       '2018-01-31', '2018-02-28', '2018-03-29', '2018-04-30',\n",
              "       '2018-05-31', '2018-06-29', '2018-07-31', '2018-08-31',\n",
              "       '2018-09-28', '2018-10-31', '2018-11-30', '2018-12-31',\n",
              "       '2019-01-31', '2019-02-28', '2019-03-29', '2019-04-30',\n",
              "       '2019-05-31', '2019-06-28', '2019-07-31', '2019-08-30',\n",
              "       '2019-09-30', '2019-10-31', '2019-11-29', '2019-12-31',\n",
              "       '2020-01-31', '2020-02-28', '2020-03-31', '2020-04-30',\n",
              "       '2020-05-29', '2020-06-30', '2020-07-31', '2020-08-31',\n",
              "       '2020-09-30', '2020-10-30', '2020-11-30', '2020-12-31',\n",
              "       '2021-01-29', '2021-02-26', '2021-03-31', '2021-04-30',\n",
              "       '2021-05-28', '2021-06-30', '2021-07-30', '2021-08-31',\n",
              "       '2021-09-30', '2021-10-29', '2021-11-30', '2021-12-31',\n",
              "       '2022-01-31', '2022-02-28', '2022-03-31', '2022-04-29',\n",
              "       '2022-05-31', '2022-06-30', '2022-07-29', '2022-08-31',\n",
              "       '2022-09-30', '2022-10-31', '2022-11-30', '2022-12-30'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare Dataset"
      ],
      "metadata": {
        "id": "dOetoTv5Izdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get multifactor list\n",
        "multifactor_list = Technical_list+fundamental_list\n",
        "\n",
        "# get x_dataset, y_dataset\n",
        "multi_factor_data = multi_factor_data.astype(float)\n",
        "label_df = label_df.astype(float)\n",
        "x_dataset = []\n",
        "y_dataset = []\n",
        "\n",
        "for ticker in tqdm(SP500_ticker):\n",
        "  y_dataset.append(label_df[ticker])\n",
        "  #print(ticker)\n",
        "  for factor in multifactor_list:\n",
        "    x_dataset.append(multi_factor_data[ticker, factor])\n",
        "\n",
        "y_dataset = np.array(y_dataset).T\n",
        "x_dataset = np.array(x_dataset).T\n",
        "\n",
        "print('y_dataset',y_dataset.shape)\n",
        "print('x_dataset',x_dataset.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "df137d18c19945d29b8e6dcbb690c366",
            "4fde3028a59449bd8d5a4bafa6221f2c",
            "c7b894306c06446381cb949751c18c20",
            "a49a81ad051b4d7aab7ac67c5539968e",
            "6c35dadbd47b44338f5a2d4fc3842c90",
            "ca02c0b7e75c468da13d64e74cc83fb3",
            "af69eb75294d4ddf908287ee58ad9c2f",
            "a211d5ca2fc4477186d9319a83ec31fa",
            "59df5c89bd24497a862f5100f2492308",
            "b6af157aca6e4521ade9d2cf05854f03",
            "e220997d63bf49498b499dd31ac0bdac"
          ]
        },
        "id": "BSnejP-WCup-",
        "outputId": "89eb0b6f-0faa-4e27-e913-aeb9f0626b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df137d18c19945d29b8e6dcbb690c366"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_dataset (276, 500)\n",
            "x_dataset (276, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_future = 1   # Number of month we want top predict into the future\n",
        "n_past = 3  "
      ],
      "metadata": {
        "id": "DibDIeSfCw2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "x_dataset = scaler.fit_transform(x_dataset)\n",
        "y_dataset = scaler.fit_transform(y_dataset)\n",
        "x_dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrWN2Z38CzdW",
        "outputId": "231040aa-7c0c-4440-9184-b59703a0179a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(276, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparation of training set\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i in range(n_past, len(x_dataset) - n_future +1):\n",
        "  #print(i)\n",
        "  X.append(x_dataset[i - n_past:i,:])\n",
        "Y = y_dataset[n_past:]\n",
        "X, Y = np.array(X), np.array(Y)\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdFMCsyKC7aI",
        "outputId": "f03f224c-f796-4729-fdb9-9fbdc5bb54a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(273, 3, 10000)\n",
            "(273, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_point = X.shape[0]-26\n",
        "train_point"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFKmGgLKH04P",
        "outputId": "928ed43e-4d9c-4c40-97c2-9be6390040cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "247"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparation of test set\n",
        "x_train = X[0:train_point]\n",
        "y_train = Y[0:train_point]\n",
        "\n",
        "x_test = X[train_point:]\n",
        "y_test = Y[train_point:]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZrezQkaC-8E",
        "outputId": "6b77dc80-aeb7-4813-9b84-efd2a3bc8b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(247, 3, 10000)\n",
            "(247, 500)\n",
            "(26, 3, 10000)\n",
            "(26, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM"
      ],
      "metadata": {
        "id": "eeFwOAQVIqJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Neural Network based on LSTM\n",
        "model = Sequential()\n",
        "\n",
        "# Adding 1st LSTM layer\n",
        "model.add(LSTM(units=1024, return_sequences=True, input_shape=(n_past, x_dataset.shape[1])))\n",
        "\n",
        "# Adding 2nd LSTM layer\n",
        "model.add(LSTM(units=1024, return_sequences=False))\n",
        "\n",
        "# Adding Dropout\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(units=512, activation='linear'))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(units=500, activation='linear'))\n",
        "\n",
        "# Compiling the Neural Network\n",
        "model.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error')\n",
        "#model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
        "mcp = ModelCheckpoint(filepath='/content/drive/My Drive/FYP Data/weights_w_fundamental_1.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)\n",
        "\n",
        "tb = TensorBoard('logs')\n",
        "\n",
        "history = model.fit(x_train, y_train, shuffle=True, epochs=10, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNaPksStDMPQ",
        "outputId": "eb7c0ea8-8ac7-4602-d377-6edd95b6ddbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "14/20 [====================>.........] - ETA: 0s - loss: 29.1203\n",
            "Epoch 1: val_loss improved from inf to 9.08739, saving model to /content/drive/My Drive/FYP Data/weights_w_fundamental_1.h5\n",
            "20/20 [==============================] - 7s 224ms/step - loss: 25.6217 - val_loss: 9.0874 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 5.3655\n",
            "Epoch 2: val_loss improved from 9.08739 to 0.56369, saving model to /content/drive/My Drive/FYP Data/weights_w_fundamental_1.h5\n",
            "20/20 [==============================] - 2s 93ms/step - loss: 4.5484 - val_loss: 0.5637 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 1.3031\n",
            "Epoch 3: val_loss improved from 0.56369 to 0.45926, saving model to /content/drive/My Drive/FYP Data/weights_w_fundamental_1.h5\n",
            "20/20 [==============================] - 4s 214ms/step - loss: 1.2116 - val_loss: 0.4593 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 0.6330\n",
            "Epoch 4: val_loss improved from 0.45926 to 0.20465, saving model to /content/drive/My Drive/FYP Data/weights_w_fundamental_1.h5\n",
            "20/20 [==============================] - 2s 93ms/step - loss: 0.6080 - val_loss: 0.2047 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 0.5221\n",
            "Epoch 5: val_loss did not improve from 0.20465\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5213 - val_loss: 0.3452 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 0.5059\n",
            "Epoch 6: val_loss improved from 0.20465 to 0.20109, saving model to /content/drive/My Drive/FYP Data/weights_w_fundamental_1.h5\n",
            "20/20 [==============================] - 2s 89ms/step - loss: 0.4833 - val_loss: 0.2011 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 0.3658\n",
            "Epoch 7: val_loss did not improve from 0.20109\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.3642 - val_loss: 0.2270 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 0.3544\n",
            "Epoch 8: val_loss did not improve from 0.20109\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.3521 - val_loss: 0.2084 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 0.3376\n",
            "Epoch 9: val_loss improved from 0.20109 to 0.19941, saving model to /content/drive/My Drive/FYP Data/weights_w_fundamental_1.h5\n",
            "20/20 [==============================] - 2s 127ms/step - loss: 0.3353 - val_loss: 0.1994 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 0.3187\n",
            "Epoch 10: val_loss did not improve from 0.19941\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.3167 - val_loss: 0.2254 - lr: 0.0100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "predictions = model.predict(x_test)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
        "print('rmse', rmse)\n",
        "\n",
        "#profit test\n",
        "test_period = time[n_past+train_point:]\n",
        "\n",
        "#calculate the top 10 stock we picked\n",
        "stock_selection_index = np.argsort(predictions)[:,-30:]\n",
        "stock_selection = []\n",
        "for index in stock_selection_index:\n",
        "  stock_list = []\n",
        "  for stock_i in index:\n",
        "    stock_list.append(SP500_ticker[stock_i])\n",
        "  stock_selection.append(stock_list)\n",
        "\n",
        "#trading simulate\n",
        "original_money = 0\n",
        "trading = 0\n",
        "for stock in stock_selection[0]:\n",
        "  original_money = original_money + stock_data[stock][\"Close\"][test_period[0]]\n",
        "  #print(stock_data[stock][\"Close\"][test_period[0]])\n",
        "\n",
        "\n",
        "trading = - original_money\n",
        "i = 0\n",
        "\n",
        "for stock_list in stock_selection:\n",
        "  if i==0 or 25:\n",
        "    i = i + 1\n",
        "    continue\n",
        "\n",
        "  for stock in stock_list:\n",
        "    trading = trading + stock_data[stock][\"Close\"][test_period[i-1]] - stock_data[stock][\"Close\"][test_period[i]]\n",
        "    #print(trading)\n",
        "  \n",
        "  i = i + 1\n",
        "\n",
        "end_money = 0\n",
        "\n",
        "for stock in stock_selection[-1]:\n",
        "  trading = trading + stock_data[stock][\"Close\"][test_period[-1]]\n",
        "  end_money = end_money + stock_data[stock][\"Close\"][test_period[-1]]\n",
        "  #print(stock_data[stock][\"Close\"][test_period[-1]])\n",
        "\n",
        "print('The money earned by LSTM with Fundamental and Technical data is ',trading)\n",
        "print('The money earned by LSTM with Fundamental and Technical data is %',trading*100/original_money)\n",
        "print('The money earned by LSTM with Fundamental and Technical data is %(unchange)',(end_money-original_money)*100/original_money)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guTsaQ--DM9Q",
        "outputId": "e86d89d2-0e7e-415f-b3d4-4bca0fb4bbf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 648ms/step\n",
            "rmse 0.011940589107573032\n",
            "The money earned by LSTM with Fundamental and Technical data is  1704.2148513793945\n",
            "The money earned by LSTM with Fundamental and Technical data is % 27.10763248665312\n",
            "The money earned by LSTM with Fundamental and Technical data is %(unchange) 27.10763248665312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GRU"
      ],
      "metadata": {
        "id": "0BvLhRlwIjOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# The GRU architecture\n",
        "regressorGRU = Sequential()\n",
        "# First GRU layer with Dropout regularisation\n",
        "regressorGRU.add(GRU(units=1024, return_sequences=True, input_shape=(n_past, x_dataset.shape[1]), activation='tanh'))\n",
        "regressorGRU.add(Dropout(0.2))\n",
        "# Second GRU layer\n",
        "regressorGRU.add(GRU(units=512, return_sequences=True, input_shape=(n_past, x_dataset.shape[1]), activation='tanh'))\n",
        "regressorGRU.add(Dropout(0.2))\n",
        "# Third GRU layer\n",
        "regressorGRU.add(GRU(units=500, return_sequences=True, input_shape=(n_past, x_dataset.shape[1]), activation='tanh'))\n",
        "regressorGRU.add(Dropout(0.2))\n",
        "# Fourth GRU layer\n",
        "regressorGRU.add(GRU(units=500, activation='tanh'))\n",
        "regressorGRU.add(Dropout(0.2))\n",
        "# The output layer\n",
        "regressorGRU.add(Dense(units=500))\n",
        "\n",
        "# The output layer\n",
        "regressorGRU.add(Dense(units=500))\n",
        "\n",
        "# Compiling the RNN\n",
        "regressorGRU.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error')\n",
        "#regressorGRU.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error')\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
        "mcp = ModelCheckpoint(filepath='/content/drive/My Drive/FYP Data/weights_GRU.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)\n",
        "\n",
        "tb = TensorBoard('logs')\n",
        "\n",
        "# Fitting to the training set\n",
        "history = regressorGRU.fit(x_train, y_train, shuffle=True, epochs=10, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMmhUEeGEZlb",
        "outputId": "a2784756-4a24-4dc6-b21a-aec73864e8fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 10.9014\n",
            "Epoch 1: val_loss improved from inf to 0.38808, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 11s 291ms/step - loss: 10.9014 - val_loss: 0.3881 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 2.6001\n",
            "Epoch 2: val_loss did not improve from 0.38808\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 2.5317 - val_loss: 1.8783 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 1.3784\n",
            "Epoch 3: val_loss improved from 0.38808 to 0.32041, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 1s 72ms/step - loss: 1.3407 - val_loss: 0.3204 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "14/20 [====================>.........] - ETA: 0s - loss: 1.1432\n",
            "Epoch 4: val_loss did not improve from 0.32041\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 1.0297 - val_loss: 0.4548 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 0.5113\n",
            "Epoch 5: val_loss did not improve from 0.32041\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.4908 - val_loss: 0.3566 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 0.4824\n",
            "Epoch 6: val_loss did not improve from 0.32041\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.4629 - val_loss: 0.3506 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3231\n",
            "Epoch 7: val_loss improved from 0.32041 to 0.19528, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 1s 70ms/step - loss: 0.3231 - val_loss: 0.1953 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2564\n",
            "Epoch 8: val_loss did not improve from 0.19528\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2564 - val_loss: 0.1972 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "14/20 [====================>.........] - ETA: 0s - loss: 0.2454\n",
            "Epoch 9: val_loss did not improve from 0.19528\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2450 - val_loss: 0.2016 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 0.2443\n",
            "Epoch 10: val_loss did not improve from 0.19528\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2426 - val_loss: 0.1969 - lr: 0.0100\n",
            "CPU times: user 12.9 s, sys: 1.32 s, total: 14.2 s\n",
            "Wall time: 17.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "predictions = regressorGRU.predict(x_test)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
        "print('rmse', rmse)\n",
        "\n",
        "#profit test\n",
        "test_period = time[n_past+245:]\n",
        "\n",
        "#calculate the top 10 stock we picked\n",
        "stock_selection_index = np.argsort(predictions)[:,-30:]\n",
        "stock_selection = []\n",
        "for index in stock_selection_index:\n",
        "  stock_list = []\n",
        "  for stock_i in index:\n",
        "    stock_list.append(SP500_ticker[stock_i])\n",
        "  stock_selection.append(stock_list)\n",
        "\n",
        "#trading simulate\n",
        "original_money = 0\n",
        "trading = 0\n",
        "for stock in stock_selection[0]:\n",
        "  original_money = original_money + stock_data[stock][\"Close\"][test_period[0]]\n",
        "  #print(stock_data[stock][\"Close\"][test_period[0]])\n",
        "\n",
        "\n",
        "trading = - original_money\n",
        "i = 0\n",
        "\n",
        "for stock_list in stock_selection:\n",
        "  if i==0 or 25:\n",
        "    i = i + 1\n",
        "    continue\n",
        "\n",
        "  for stock in stock_list:\n",
        "    trading = trading + stock_data[stock][\"Close\"][test_period[i-1]] - stock_data[stock][\"Close\"][test_period[i]]\n",
        "    #print(trading)\n",
        "  \n",
        "  i = i + 1\n",
        "\n",
        "end_money = 0\n",
        "\n",
        "for stock in stock_selection[-1]:\n",
        "  trading = trading + stock_data[stock][\"Close\"][test_period[-1]]\n",
        "  end_money = end_money + stock_data[stock][\"Close\"][test_period[-1]]\n",
        "  #print(stock_data[stock][\"Close\"][test_period[-1]])\n",
        "\n",
        "print('The money earned by LSTM with Fundamental and Technical data is ',trading)\n",
        "print('The money earned by LSTM with Fundamental and Technical data is %',trading*100/original_money)\n",
        "print('The money earned by LSTM with Fundamental and Technical data is %(unchange)',(end_money-original_money)*100/original_money)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZRZS4fNEwaR",
        "outputId": "1719fe3b-de08-40b4-fef2-9f35bdc13608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "rmse 0.025873210886063484\n",
            "The money earned by LSTM with Fundamental and Technical data is  660.0535297393799\n",
            "The money earned by LSTM with Fundamental and Technical data is % 15.001348028290357\n",
            "The money earned by LSTM with Fundamental and Technical data is %(unchange) 15.001348028290357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train with past 5 years"
      ],
      "metadata": {
        "id": "3TM6JBkMIlXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_future = 1   # Number of month we want top predict into the future\n",
        "n_past = 5  \n",
        "\n",
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "x_dataset = scaler.fit_transform(x_dataset)\n",
        "y_dataset = scaler.fit_transform(y_dataset)\n",
        "\n",
        "#Preparation of training set\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i in range(n_past, len(x_dataset) - n_future +1):\n",
        "  #print(i)\n",
        "  X.append(x_dataset[i - n_past:i,:])\n",
        "Y = y_dataset[n_past:]\n",
        "X, Y = np.array(X), np.array(Y)\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-GiHHn-Ic3Z",
        "outputId": "ea149c87-056d-470f-bf10-4bfef5c99e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(271, 5, 10000)\n",
            "(271, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_point = X.shape[0]-26\n",
        "\n",
        "#Preparation of test set\n",
        "x_train = X[0:train_point]\n",
        "y_train = Y[0:train_point]\n",
        "\n",
        "x_test = X[train_point:]\n",
        "y_test = Y[train_point:]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ErIsklxId20",
        "outputId": "d7cc8c3b-5317-4dd2-94e6-7264a48e4cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(245, 5, 10000)\n",
            "(245, 500)\n",
            "(26, 5, 10000)\n",
            "(26, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM"
      ],
      "metadata": {
        "id": "RUTwsuHAJhLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Neural Network based on LSTM\n",
        "model = Sequential()\n",
        "\n",
        "# Adding 1st LSTM layer\n",
        "model.add(LSTM(units=1024, return_sequences=True, input_shape=(n_past, x_dataset.shape[1])))\n",
        "\n",
        "# Adding 2nd LSTM layer\n",
        "model.add(LSTM(units=1024, return_sequences=False))\n",
        "\n",
        "# Adding Dropout\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(units=512, activation='linear'))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(units=500, activation='linear'))\n",
        "\n",
        "# Compiling the Neural Network\n",
        "model.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error')\n",
        "#model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
        "mcp = ModelCheckpoint(filepath='/content/drive/My Drive/FYP Data/weights_w_fundamental_1.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)\n",
        "\n",
        "tb = TensorBoard('logs')\n",
        "\n",
        "history = model.fit(x_train, y_train, shuffle=True, epochs=10, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=10)\n",
        "\n",
        "#Model Evaluation\n",
        "predictions = model.predict(x_test)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
        "print('rmse', rmse)\n",
        "\n",
        "#profit test\n",
        "test_period = time[n_past+train_point:]\n",
        "\n",
        "#calculate the top 10 stock we picked\n",
        "stock_selection_index = np.argsort(predictions)[:,-30:]\n",
        "stock_selection = []\n",
        "for index in stock_selection_index:\n",
        "  stock_list = []\n",
        "  for stock_i in index:\n",
        "    stock_list.append(SP500_ticker[stock_i])\n",
        "  stock_selection.append(stock_list)\n",
        "\n",
        "#trading simulate\n",
        "original_money = 0\n",
        "trading = 0\n",
        "for stock in stock_selection[0]:\n",
        "  original_money = original_money + stock_data[stock][\"Close\"][test_period[0]]\n",
        "  #print(stock_data[stock][\"Close\"][test_period[0]])\n",
        "\n",
        "\n",
        "trading = - original_money\n",
        "i = 0\n",
        "\n",
        "for stock_list in stock_selection:\n",
        "  if i==0 or 25:\n",
        "    i = i + 1\n",
        "    continue\n",
        "\n",
        "  for stock in stock_list:\n",
        "    trading = trading + stock_data[stock][\"Close\"][test_period[i-1]] - stock_data[stock][\"Close\"][test_period[i]]\n",
        "    #print(trading)\n",
        "  \n",
        "  i = i + 1\n",
        "\n",
        "end_money = 0\n",
        "\n",
        "for stock in stock_selection[-1]:\n",
        "  trading = trading + stock_data[stock][\"Close\"][test_period[-1]]\n",
        "  end_money = end_money + stock_data[stock][\"Close\"][test_period[-1]]\n",
        "  #print(stock_data[stock][\"Close\"][test_period[-1]])\n",
        "\n",
        "print('The money earned by LSTM with Fundamental and Technical data is ',trading)\n",
        "print('The money earned by LSTM with Fundamental and Technical data is %',trading*100/original_money)\n",
        "print('The money earned by LSTM with Fundamental and Technical data is %(unchange)',(end_money-original_money)*100/original_money)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDc5tMQQJITQ",
        "outputId": "1cb1b845-b22d-4569-ce08-0205813974b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "14/20 [====================>.........] - ETA: 0s - loss: 13.4692\n",
            "Epoch 1: val_loss improved from inf to 0.44177, saving model to /content/drive/My Drive/FYP Data/weights_w_fundamental_1.h5\n",
            "20/20 [==============================] - 8s 292ms/step - loss: 12.1928 - val_loss: 0.4418 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 4.6328\n",
            "Epoch 2: val_loss did not improve from 0.44177\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 4.6507 - val_loss: 1.4699 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "14/20 [====================>.........] - ETA: 0s - loss: 1.1958\n",
            "Epoch 3: val_loss improved from 0.44177 to 0.38359, saving model to /content/drive/My Drive/FYP Data/weights_w_fundamental_1.h5\n",
            "20/20 [==============================] - 2s 92ms/step - loss: 1.0972 - val_loss: 0.3836 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 0.5284\n",
            "Epoch 4: val_loss improved from 0.38359 to 0.23352, saving model to /content/drive/My Drive/FYP Data/weights_w_fundamental_1.h5\n",
            "20/20 [==============================] - 2s 95ms/step - loss: 0.4991 - val_loss: 0.2335 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "14/20 [====================>.........] - ETA: 0s - loss: 0.3620\n",
            "Epoch 5: val_loss improved from 0.23352 to 0.20487, saving model to /content/drive/My Drive/FYP Data/weights_w_fundamental_1.h5\n",
            "20/20 [==============================] - 2s 94ms/step - loss: 0.3557 - val_loss: 0.2049 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3270\n",
            "Epoch 6: val_loss did not improve from 0.20487\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3270 - val_loss: 0.2066 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 0.3108\n",
            "Epoch 7: val_loss did not improve from 0.20487\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3103 - val_loss: 0.2181 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 0.3037\n",
            "Epoch 8: val_loss improved from 0.20487 to 0.20425, saving model to /content/drive/My Drive/FYP Data/weights_w_fundamental_1.h5\n",
            "20/20 [==============================] - 3s 177ms/step - loss: 0.3029 - val_loss: 0.2042 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2885\n",
            "Epoch 9: val_loss did not improve from 0.20425\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2885 - val_loss: 0.2052 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "15/20 [=====================>........] - ETA: 0s - loss: 0.2822\n",
            "Epoch 10: val_loss did not improve from 0.20425\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2820 - val_loss: 0.2046 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "rmse 0.0034615060687065123\n",
            "The money earned by LSTM with Fundamental and Technical data is  1260.2059535980225\n",
            "The money earned by LSTM with Fundamental and Technical data is % 12.229174710802258\n",
            "The money earned by LSTM with Fundamental and Technical data is %(unchange) 12.229174710802258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GRU"
      ],
      "metadata": {
        "id": "si2GNGIHJeL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# The GRU architecture\n",
        "regressorGRU = Sequential()\n",
        "# First GRU layer with Dropout regularisation\n",
        "regressorGRU.add(GRU(units=1024, return_sequences=True, input_shape=(n_past, x_dataset.shape[1]), activation='tanh'))\n",
        "regressorGRU.add(Dropout(0.2))\n",
        "# Second GRU layer\n",
        "regressorGRU.add(GRU(units=512, return_sequences=True, input_shape=(n_past, x_dataset.shape[1]), activation='tanh'))\n",
        "regressorGRU.add(Dropout(0.2))\n",
        "# Third GRU layer\n",
        "regressorGRU.add(GRU(units=500, return_sequences=True, input_shape=(n_past, x_dataset.shape[1]), activation='tanh'))\n",
        "regressorGRU.add(Dropout(0.2))\n",
        "# Fourth GRU layer\n",
        "regressorGRU.add(GRU(units=500, activation='tanh'))\n",
        "regressorGRU.add(Dropout(0.2))\n",
        "# The output layer\n",
        "regressorGRU.add(Dense(units=500))\n",
        "\n",
        "# The output layer\n",
        "regressorGRU.add(Dense(units=500))\n",
        "\n",
        "# Compiling the RNN\n",
        "regressorGRU.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error')\n",
        "#regressorGRU.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error')\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
        "mcp = ModelCheckpoint(filepath='/content/drive/My Drive/FYP Data/weights_GRU.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)\n",
        "\n",
        "tb = TensorBoard('logs')\n",
        "\n",
        "# Fitting to the training set\n",
        "history = regressorGRU.fit(x_train, y_train, shuffle=True, epochs=10, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=10)\n",
        "\n",
        "\n",
        "#Model Evaluation\n",
        "predictions = regressorGRU.predict(x_test)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
        "print('rmse', rmse)\n",
        "\n",
        "#profit test\n",
        "test_period = time[n_past+245:]\n",
        "\n",
        "#calculate the top 10 stock we picked\n",
        "stock_selection_index = np.argsort(predictions)[:,-30:]\n",
        "stock_selection = []\n",
        "for index in stock_selection_index:\n",
        "  stock_list = []\n",
        "  for stock_i in index:\n",
        "    stock_list.append(SP500_ticker[stock_i])\n",
        "  stock_selection.append(stock_list)\n",
        "\n",
        "#trading simulate\n",
        "original_money = 0\n",
        "trading = 0\n",
        "for stock in stock_selection[0]:\n",
        "  original_money = original_money + stock_data[stock][\"Close\"][test_period[0]]\n",
        "  #print(stock_data[stock][\"Close\"][test_period[0]])\n",
        "\n",
        "\n",
        "trading = - original_money\n",
        "i = 0\n",
        "\n",
        "for stock_list in stock_selection:\n",
        "  if i==0 or 25:\n",
        "    i = i + 1\n",
        "    continue\n",
        "\n",
        "  for stock in stock_list:\n",
        "    trading = trading + stock_data[stock][\"Close\"][test_period[i-1]] - stock_data[stock][\"Close\"][test_period[i]]\n",
        "    #print(trading)\n",
        "  \n",
        "  i = i + 1\n",
        "\n",
        "end_money = 0\n",
        "\n",
        "for stock in stock_selection[-1]:\n",
        "  trading = trading + stock_data[stock][\"Close\"][test_period[-1]]\n",
        "  end_money = end_money + stock_data[stock][\"Close\"][test_period[-1]]\n",
        "  #print(stock_data[stock][\"Close\"][test_period[-1]])\n",
        "\n",
        "print('The money earned by LSTM with Fundamental and Technical data is ',trading)\n",
        "print('The money earned by LSTM with Fundamental and Technical data is %',trading*100/original_money)\n",
        "print('The money earned by LSTM with Fundamental and Technical data is %(unchange)',(end_money-original_money)*100/original_money)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83z2M2AHJUaR",
        "outputId": "ed727971-99e7-4d72-bd98-2d7816406fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 6.3923\n",
            "Epoch 1: val_loss improved from inf to 4.44832, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 11s 272ms/step - loss: 6.2562 - val_loss: 4.4483 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 2.7716\n",
            "Epoch 2: val_loss improved from 4.44832 to 1.11747, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 1s 75ms/step - loss: 2.7163 - val_loss: 1.1175 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "17/20 [========================>.....] - ETA: 0s - loss: 0.8413\n",
            "Epoch 3: val_loss improved from 1.11747 to 0.21432, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 1s 77ms/step - loss: 0.7934 - val_loss: 0.2143 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "18/20 [==========================>...] - ETA: 0s - loss: 0.4075\n",
            "Epoch 4: val_loss did not improve from 0.21432\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.3994 - val_loss: 0.2178 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3106\n",
            "Epoch 5: val_loss improved from 0.21432 to 0.20162, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.3103 - val_loss: 0.2016 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2804\n",
            "Epoch 6: val_loss improved from 0.20162 to 0.19771, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 4s 185ms/step - loss: 0.2799 - val_loss: 0.1977 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "18/20 [==========================>...] - ETA: 0s - loss: 0.2572\n",
            "Epoch 7: val_loss did not improve from 0.19771\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2561 - val_loss: 0.2040 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "17/20 [========================>.....] - ETA: 0s - loss: 0.2425\n",
            "Epoch 8: val_loss did not improve from 0.19771\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2415 - val_loss: 0.1979 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2310\n",
            "Epoch 9: val_loss improved from 0.19771 to 0.19710, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 1s 75ms/step - loss: 0.2310 - val_loss: 0.1971 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "18/20 [==========================>...] - ETA: 0s - loss: 0.2271\n",
            "Epoch 10: val_loss did not improve from 0.19710\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2266 - val_loss: 0.2010 - lr: 0.0100\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "rmse 0.017999564815312626\n",
            "The money earned by LSTM with Fundamental and Technical data is  166.1531467437744\n",
            "The money earned by LSTM with Fundamental and Technical data is % 2.720231413108065\n",
            "The money earned by LSTM with Fundamental and Technical data is %(unchange) 2.720231413108065\n",
            "CPU times: user 15.7 s, sys: 2.48 s, total: 18.2 s\n",
            "Wall time: 25.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stock_selection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx5W1J2MJqdK",
        "outputId": "4c35e3ed-03b3-4766-d674-c411bbbafca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'REGN',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX']]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#rolling strategies"
      ],
      "metadata": {
        "id": "IBHCPLWnJucN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
        "mcp = ModelCheckpoint(filepath='/content/drive/My Drive/FYP Data/weights_GRU_fintune.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)\n",
        "predictions = []\n",
        "\n",
        "for i in tqdm(range(26)):\n",
        "  if i == 0:\n",
        "    GRU_model = keras.models.load_model('/content/drive/My Drive/FYP Data/weights_GRU.h5')\n",
        "  else:\n",
        "    GRU_model = keras.models.load_model('/content/drive/My Drive/FYP Data/weights_GRU_fintune.h5')\n",
        "  prediction = GRU_model.predict(x_test[i:i+1])\n",
        "  predictions.append(prediction)\n",
        "  x_train = X[i:train_point+i]\n",
        "  y_train = Y[i:train_point+i]\n",
        "\n",
        "  #fintune model\n",
        "  history = GRU_model.fit(x_train, y_train, shuffle=True, epochs=10, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "32203cbb24a64951996d07990691e92e",
            "5fca29a65f914059b798fed69c2f66c1",
            "8e5e99afab8241c3ac83fc93f06f7483",
            "8017916114474b07bbf8d7fe723f3670",
            "17dd5ff635cd48b984969ba792d6fd57",
            "78acf403d4934bca8a92bd04bcd97155",
            "7fa53c6d694f4ef293e1ed638a068da6",
            "877aa45ee28c421cac99cd15c1571e28",
            "4ae562f559e34615b74407017b83b3f5",
            "912e9d30a0e14488a990259d500f060c",
            "1a93342296de4696950d4574dd88b613"
          ]
        },
        "id": "e9EfMPInJwRl",
        "outputId": "d52f6e9c-7d8f-4b7a-80b8-a45aed24e068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/26 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32203cbb24a64951996d07990691e92e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "18/20 [==========================>...] - ETA: 0s - loss: 0.2734\n",
            "Epoch 1: val_loss improved from inf to 0.21504, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 6s 132ms/step - loss: 0.2721 - val_loss: 0.2150 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2725\n",
            "Epoch 2: val_loss did not improve from 0.21504\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2720 - val_loss: 0.2169 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2675\n",
            "Epoch 3: val_loss improved from 0.21504 to 0.21045, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 1s 72ms/step - loss: 0.2668 - val_loss: 0.2105 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2568\n",
            "Epoch 4: val_loss improved from 0.21045 to 0.20054, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 1s 73ms/step - loss: 0.2564 - val_loss: 0.2005 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2463\n",
            "Epoch 5: val_loss did not improve from 0.20054\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2461 - val_loss: 0.2057 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2414\n",
            "Epoch 6: val_loss did not improve from 0.20054\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2412 - val_loss: 0.2018 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2360\n",
            "Epoch 7: val_loss improved from 0.20054 to 0.19881, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 1s 72ms/step - loss: 0.2358 - val_loss: 0.1988 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2294\n",
            "Epoch 8: val_loss improved from 0.19881 to 0.19658, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 1s 72ms/step - loss: 0.2292 - val_loss: 0.1966 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2256\n",
            "Epoch 9: val_loss did not improve from 0.19658\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2259 - val_loss: 0.1982 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2221\n",
            "Epoch 10: val_loss did not improve from 0.19658\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2218 - val_loss: 0.2029 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2282\n",
            "Epoch 1: val_loss did not improve from 0.19658\n",
            "20/20 [==============================] - 5s 72ms/step - loss: 0.2279 - val_loss: 0.2073 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2238\n",
            "Epoch 2: val_loss did not improve from 0.19658\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2238 - val_loss: 0.2013 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2204\n",
            "Epoch 3: val_loss improved from 0.19658 to 0.19645, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 1s 72ms/step - loss: 0.2206 - val_loss: 0.1965 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2210\n",
            "Epoch 4: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2210 - val_loss: 0.2005 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2200\n",
            "Epoch 5: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2200 - val_loss: 0.1976 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2155\n",
            "Epoch 6: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2156 - val_loss: 0.2024 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "18/20 [==========================>...] - ETA: 0s - loss: 0.2178\n",
            "Epoch 7: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2173 - val_loss: 0.2005 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2154\n",
            "Epoch 8: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2157 - val_loss: 0.2010 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2148\n",
            "Epoch 9: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2147 - val_loss: 0.1985 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2120\n",
            "Epoch 10: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2121 - val_loss: 0.2004 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2181\n",
            "Epoch 1: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 5s 74ms/step - loss: 0.2181 - val_loss: 0.1993 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2183\n",
            "Epoch 2: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2179 - val_loss: 0.1995 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2147\n",
            "Epoch 3: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2149 - val_loss: 0.2010 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2149\n",
            "Epoch 4: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2151 - val_loss: 0.1992 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2136\n",
            "Epoch 5: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2135 - val_loss: 0.1966 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2137\n",
            "Epoch 6: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2140 - val_loss: 0.1984 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2134\n",
            "Epoch 7: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2130 - val_loss: 0.2013 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2127\n",
            "Epoch 8: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2133 - val_loss: 0.1995 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2115\n",
            "Epoch 9: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2115 - val_loss: 0.2073 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2156\n",
            "Epoch 10: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2159 - val_loss: 0.2137 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2187\n",
            "Epoch 1: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 5s 74ms/step - loss: 0.2187 - val_loss: 0.1991 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2196\n",
            "Epoch 2: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2193 - val_loss: 0.1985 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2153\n",
            "Epoch 3: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2152 - val_loss: 0.2028 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2153\n",
            "Epoch 4: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2152 - val_loss: 0.2067 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2150\n",
            "Epoch 5: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2148 - val_loss: 0.1996 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2112\n",
            "Epoch 6: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2115 - val_loss: 0.2019 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2131\n",
            "Epoch 7: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2129 - val_loss: 0.1972 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2090\n",
            "Epoch 8: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2090 - val_loss: 0.1981 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2103\n",
            "Epoch 9: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2106 - val_loss: 0.1993 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2091\n",
            "Epoch 10: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2090 - val_loss: 0.2001 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2178\n",
            "Epoch 1: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 5s 71ms/step - loss: 0.2175 - val_loss: 0.1973 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2169\n",
            "Epoch 2: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2168 - val_loss: 0.1994 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "18/20 [==========================>...] - ETA: 0s - loss: 0.2155\n",
            "Epoch 3: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2155 - val_loss: 0.1988 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2145\n",
            "Epoch 4: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2143 - val_loss: 0.1994 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2134\n",
            "Epoch 5: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2135 - val_loss: 0.1997 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2117\n",
            "Epoch 6: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2116 - val_loss: 0.1990 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2109\n",
            "Epoch 7: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2107 - val_loss: 0.1977 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2089\n",
            "Epoch 8: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2090 - val_loss: 0.2026 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2114\n",
            "Epoch 9: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2114 - val_loss: 0.1989 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2108\n",
            "Epoch 10: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2113 - val_loss: 0.1968 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2218\n",
            "Epoch 1: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 5s 114ms/step - loss: 0.2217 - val_loss: 0.2034 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2212\n",
            "Epoch 2: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2214 - val_loss: 0.2238 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2166\n",
            "Epoch 3: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2165 - val_loss: 0.2004 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2172\n",
            "Epoch 4: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2168 - val_loss: 0.2005 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2150\n",
            "Epoch 5: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2157 - val_loss: 0.2049 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2166\n",
            "Epoch 6: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2164 - val_loss: 0.1977 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2109\n",
            "Epoch 7: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2113 - val_loss: 0.1988 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2099\n",
            "Epoch 8: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2096 - val_loss: 0.2008 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2104\n",
            "Epoch 9: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2104 - val_loss: 0.1988 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2089\n",
            "Epoch 10: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2089 - val_loss: 0.2041 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2197\n",
            "Epoch 1: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 5s 71ms/step - loss: 0.2196 - val_loss: 0.2154 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2221\n",
            "Epoch 2: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2222 - val_loss: 0.2141 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2190\n",
            "Epoch 3: val_loss did not improve from 0.19645\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2193 - val_loss: 0.1984 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2184\n",
            "Epoch 4: val_loss improved from 0.19645 to 0.19566, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 1s 72ms/step - loss: 0.2185 - val_loss: 0.1957 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2151\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2151 - val_loss: 0.2033 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2160\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2157 - val_loss: 0.1984 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2120\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2119 - val_loss: 0.2003 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2112\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2111 - val_loss: 0.2009 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2117\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2117 - val_loss: 0.1992 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2110\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2104 - val_loss: 0.1996 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "18/20 [==========================>...] - ETA: 0s - loss: 0.2133\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 73ms/step - loss: 0.2136 - val_loss: 0.2065 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2141\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2141 - val_loss: 0.2032 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2123\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2119 - val_loss: 0.1991 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2133\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2133 - val_loss: 0.2209 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2177\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2176 - val_loss: 0.1982 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2151\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2150 - val_loss: 0.1980 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2088\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2089 - val_loss: 0.2084 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2158\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2161 - val_loss: 0.2347 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2216\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2220 - val_loss: 0.1995 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2133\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2133 - val_loss: 0.1989 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "18/20 [==========================>...] - ETA: 0s - loss: 0.2143\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 6s 74ms/step - loss: 0.2149 - val_loss: 0.1979 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2149\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2144 - val_loss: 0.1994 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2112\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2112 - val_loss: 0.2017 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2116\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2116 - val_loss: 0.1979 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2107\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2107 - val_loss: 0.2008 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2107\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2105 - val_loss: 0.1993 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2109\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2113 - val_loss: 0.1990 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2115\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2116 - val_loss: 0.2024 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2182\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2181 - val_loss: 0.2515 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2265\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2262 - val_loss: 0.1966 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2166\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 72ms/step - loss: 0.2168 - val_loss: 0.2003 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2212\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2212 - val_loss: 0.2110 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2148\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2151 - val_loss: 0.2033 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2156\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2160 - val_loss: 0.1970 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2132\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2133 - val_loss: 0.1980 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2098\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2097 - val_loss: 0.2024 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2135\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2134 - val_loss: 0.2011 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "18/20 [==========================>...] - ETA: 0s - loss: 0.2135\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2140 - val_loss: 0.2062 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2167\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2163 - val_loss: 0.2094 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2133\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2133 - val_loss: 0.2163 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2211\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 71ms/step - loss: 0.2212 - val_loss: 0.2062 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2173\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2172 - val_loss: 0.1987 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2162\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2165 - val_loss: 0.2350 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2165\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2164 - val_loss: 0.2046 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2127\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2127 - val_loss: 0.1978 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2117\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2120 - val_loss: 0.2193 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2095\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2096 - val_loss: 0.2009 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2066\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2070 - val_loss: 0.1999 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2088\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2087 - val_loss: 0.1982 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2057\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2056 - val_loss: 0.1980 - lr: 0.0100\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2127\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 73ms/step - loss: 0.2127 - val_loss: 0.2155 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2141\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2141 - val_loss: 0.1971 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2113\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2113 - val_loss: 0.1996 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2118\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2118 - val_loss: 0.2036 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2095\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2093 - val_loss: 0.1989 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2088\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2090 - val_loss: 0.2012 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2122\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2121 - val_loss: 0.2076 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2117\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2115 - val_loss: 0.2018 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2169\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2169 - val_loss: 0.2049 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2156\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2154 - val_loss: 0.2014 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2207\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 71ms/step - loss: 0.2207 - val_loss: 0.2033 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2258\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2255 - val_loss: 0.2224 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2284\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2280 - val_loss: 0.1969 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2142\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2142 - val_loss: 0.1964 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2129\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2131 - val_loss: 0.1971 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2115\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2114 - val_loss: 0.2128 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2115\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2113 - val_loss: 0.2073 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "18/20 [==========================>...] - ETA: 0s - loss: 0.2094\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2093 - val_loss: 0.1997 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2103\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2102 - val_loss: 0.2085 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2217\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2216 - val_loss: 0.2116 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2166\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 118ms/step - loss: 0.2169 - val_loss: 0.1978 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2144\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2147 - val_loss: 0.1990 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2129\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2127 - val_loss: 0.1976 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2118\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2120 - val_loss: 0.2060 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2156\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2153 - val_loss: 0.2040 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2159\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2157 - val_loss: 0.1987 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2139\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2139 - val_loss: 0.2101 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2257\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2257 - val_loss: 0.2007 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2217\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2219 - val_loss: 0.2794 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2323\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2318 - val_loss: 0.2067 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2164\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 71ms/step - loss: 0.2164 - val_loss: 0.2196 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2202\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2201 - val_loss: 0.2056 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2157\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2156 - val_loss: 0.2026 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2138\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2137 - val_loss: 0.2171 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2193\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2188 - val_loss: 0.2023 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2194\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2204 - val_loss: 0.2072 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2217\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2217 - val_loss: 0.2019 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2222\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2217 - val_loss: 0.1994 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2132\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2131 - val_loss: 0.2030 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2174\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2171 - val_loss: 0.2269 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2167\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 72ms/step - loss: 0.2169 - val_loss: 0.1986 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2138\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2137 - val_loss: 0.1977 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2113\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2115 - val_loss: 0.1974 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2109\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2107 - val_loss: 0.2001 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2090\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2090 - val_loss: 0.2091 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2087\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2085 - val_loss: 0.2007 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2096\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2095 - val_loss: 0.2049 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2071\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2071 - val_loss: 0.1986 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2068\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2066 - val_loss: 0.1973 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2072\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2074 - val_loss: 0.2079 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2155\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 6s 74ms/step - loss: 0.2155 - val_loss: 0.2032 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2151\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2151 - val_loss: 0.1965 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2111\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2112 - val_loss: 0.1984 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2114\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2111 - val_loss: 0.1995 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2120\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2123 - val_loss: 0.1991 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2092\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2094 - val_loss: 0.2010 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2101\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2103 - val_loss: 0.1980 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2142\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2140 - val_loss: 0.2341 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2166\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2163 - val_loss: 0.2239 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2329\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2321 - val_loss: 0.2309 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2197\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 71ms/step - loss: 0.2193 - val_loss: 0.2094 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2175\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2174 - val_loss: 0.2004 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2189\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2191 - val_loss: 0.2020 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2212\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2208 - val_loss: 0.2025 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2217\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2214 - val_loss: 0.1995 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2124\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2122 - val_loss: 0.2025 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2146\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2144 - val_loss: 0.1985 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2099\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2101 - val_loss: 0.1972 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2084\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2082 - val_loss: 0.2047 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2080\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2084 - val_loss: 0.1987 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2150\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 71ms/step - loss: 0.2151 - val_loss: 0.2000 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2135\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2134 - val_loss: 0.1987 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2120\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2119 - val_loss: 0.1984 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2105\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2105 - val_loss: 0.2002 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2100\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2102 - val_loss: 0.1972 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2090\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2088 - val_loss: 0.1973 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2075\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2073 - val_loss: 0.1977 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2102\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2101 - val_loss: 0.2025 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2117\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2115 - val_loss: 0.2005 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2155\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2157 - val_loss: 0.1973 - lr: 0.0100\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2143\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 72ms/step - loss: 0.2142 - val_loss: 0.1994 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2115\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2116 - val_loss: 0.1996 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2143\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2145 - val_loss: 0.1984 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2122\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2121 - val_loss: 0.2009 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2100\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2105 - val_loss: 0.2025 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2174\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2177 - val_loss: 0.2175 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2196\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2190 - val_loss: 0.2007 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2127\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2131 - val_loss: 0.1993 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2132\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2138 - val_loss: 0.2033 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2137\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2138 - val_loss: 0.2107 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2139\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 71ms/step - loss: 0.2137 - val_loss: 0.2002 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2151\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2150 - val_loss: 0.2022 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2194\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2196 - val_loss: 0.2007 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2129\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2130 - val_loss: 0.1984 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2106\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2111 - val_loss: 0.1983 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2109\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2109 - val_loss: 0.1978 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2092\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2091 - val_loss: 0.1988 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2081\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2082 - val_loss: 0.2007 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2101\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2104 - val_loss: 0.1973 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2085\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2088 - val_loss: 0.2162 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2148\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 71ms/step - loss: 0.2147 - val_loss: 0.2081 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2174\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2170 - val_loss: 0.2122 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2179\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2177 - val_loss: 0.2000 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2125\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2128 - val_loss: 0.1993 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2118\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2119 - val_loss: 0.2053 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2177\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2174 - val_loss: 0.2044 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2127\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2131 - val_loss: 0.2033 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2150\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2148 - val_loss: 0.2014 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2127\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2125 - val_loss: 0.2083 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2175\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2178 - val_loss: 0.1988 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2120\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 72ms/step - loss: 0.2123 - val_loss: 0.1982 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2144\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2142 - val_loss: 0.2160 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2182\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2183 - val_loss: 0.2006 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2169\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2169 - val_loss: 0.2016 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2157\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2159 - val_loss: 0.2143 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2251\n",
            "Epoch 6: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2251 - val_loss: 0.2195 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2258\n",
            "Epoch 7: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2254 - val_loss: 0.2131 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2126\n",
            "Epoch 8: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2131 - val_loss: 0.2007 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2179\n",
            "Epoch 9: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2179 - val_loss: 0.2088 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2179\n",
            "Epoch 10: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2179 - val_loss: 0.2116 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2165\n",
            "Epoch 1: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 5s 72ms/step - loss: 0.2165 - val_loss: 0.2033 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2209\n",
            "Epoch 2: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2206 - val_loss: 0.2041 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2127\n",
            "Epoch 3: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2130 - val_loss: 0.2014 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2139\n",
            "Epoch 4: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2139 - val_loss: 0.1996 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2098\n",
            "Epoch 5: val_loss did not improve from 0.19566\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2099 - val_loss: 0.2061 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2103\n",
            "Epoch 6: val_loss improved from 0.19566 to 0.19532, saving model to /content/drive/My Drive/FYP Data/weights_GRU.h5\n",
            "20/20 [==============================] - 1s 71ms/step - loss: 0.2102 - val_loss: 0.1953 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2090\n",
            "Epoch 7: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2092 - val_loss: 0.1969 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2091\n",
            "Epoch 8: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2090 - val_loss: 0.1990 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2120\n",
            "Epoch 9: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2122 - val_loss: 0.2007 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2160\n",
            "Epoch 10: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2161 - val_loss: 0.2003 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2082\n",
            "Epoch 1: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 6s 77ms/step - loss: 0.2080 - val_loss: 0.1994 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2113\n",
            "Epoch 2: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2113 - val_loss: 0.2047 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2080\n",
            "Epoch 3: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2080 - val_loss: 0.2055 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2153\n",
            "Epoch 4: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2153 - val_loss: 0.2036 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2120\n",
            "Epoch 5: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2121 - val_loss: 0.2015 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2149\n",
            "Epoch 6: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2146 - val_loss: 0.1988 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2098\n",
            "Epoch 7: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2102 - val_loss: 0.2306 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2123\n",
            "Epoch 8: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2120 - val_loss: 0.2274 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2107\n",
            "Epoch 9: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2108 - val_loss: 0.2024 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "18/20 [==========================>...] - ETA: 0s - loss: 0.2077\n",
            "Epoch 10: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2078 - val_loss: 0.1981 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2114\n",
            "Epoch 1: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 5s 71ms/step - loss: 0.2113 - val_loss: 0.1985 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2085\n",
            "Epoch 2: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2084 - val_loss: 0.1985 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2110\n",
            "Epoch 3: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2109 - val_loss: 0.2039 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2078\n",
            "Epoch 4: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2078 - val_loss: 0.2019 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2091\n",
            "Epoch 5: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2093 - val_loss: 0.2210 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2101\n",
            "Epoch 6: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2099 - val_loss: 0.2042 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2078\n",
            "Epoch 7: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2078 - val_loss: 0.2054 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2090\n",
            "Epoch 8: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2091 - val_loss: 0.2043 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2129\n",
            "Epoch 9: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2136 - val_loss: 0.2037 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2175\n",
            "Epoch 10: val_loss did not improve from 0.19532\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2174 - val_loss: 0.2003 - lr: 0.0100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = np.vstack([predictions[i] for i in range(26)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4CmVnBMQh76",
        "outputId": "4062a749-e173-42c0-9a08-8fc8cd97d3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
        "print('rmse', rmse)\n",
        "\n",
        "#profit test\n",
        "test_period = time[n_past+train_point:]\n",
        "\n",
        "#calculate the top 10 stock we picked\n",
        "stock_selection_index = np.argsort(predictions)[:,-30:]\n",
        "stock_selection = []\n",
        "for index in stock_selection_index:\n",
        "  stock_list = []\n",
        "  for stock_i in index:\n",
        "    stock_list.append(SP500_ticker[stock_i])\n",
        "  stock_selection.append(stock_list)\n",
        "\n",
        "#trading simulate\n",
        "original_money = 0\n",
        "trading = 0\n",
        "for stock in stock_selection[0]:\n",
        "  original_money = original_money + stock_data[stock][\"Close\"][test_period[0]]\n",
        "  #print(stock_data[stock][\"Close\"][test_period[0]])\n",
        "\n",
        "\n",
        "trading = - original_money\n",
        "i = 0\n",
        "\n",
        "for stock_list in stock_selection:\n",
        "  if i==0 or 25:\n",
        "    i = i + 1\n",
        "    continue\n",
        "\n",
        "  for stock in stock_list:\n",
        "    trading = trading + stock_data[stock][\"Close\"][test_period[i-1]] - stock_data[stock][\"Close\"][test_period[i]]\n",
        "    #print(trading)\n",
        "  \n",
        "  i = i + 1\n",
        "\n",
        "end_money = 0\n",
        "\n",
        "for stock in stock_selection[-1]:\n",
        "  trading = trading + stock_data[stock][\"Close\"][test_period[-1]]\n",
        "  end_money = end_money + stock_data[stock][\"Close\"][test_period[-1]]\n",
        "  #print(stock_data[stock][\"Close\"][test_period[-1]])\n",
        "\n",
        "print('The money earned by LSTM with Fundamental and Technical data is ',trading)\n",
        "print('The money earned by LSTM with Fundamental and Technical data is %',trading*100/original_money)\n",
        "print('The money earned by LSTM with Fundamental and Technical data is %(unchange)',(end_money-original_money)*100/original_money)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2St8oOMNpKZ",
        "outputId": "a4c59fc9-ac8f-4048-98f4-980a1cc85f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse 0.017849340602343614\n",
            "The money earned by LSTM with Fundamental and Technical data is  2194.788293838501\n",
            "The money earned by LSTM with Fundamental and Technical data is % 49.63684595275108\n",
            "The money earned by LSTM with Fundamental and Technical data is %(unchange) 49.63684595275108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stock_selection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijz6oKtBRW1m",
        "outputId": "8fb4c29a-58ef-4636-9c12-59f8fb4a532e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['VLO',\n",
              "  'LOW',\n",
              "  'NEM',\n",
              "  'ILMN',\n",
              "  'REGN',\n",
              "  'URI',\n",
              "  'BWA',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'DAL',\n",
              "  'ISRG',\n",
              "  'ALK',\n",
              "  'EQR',\n",
              "  'MAR',\n",
              "  'YUM',\n",
              "  'MNST',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'GPN',\n",
              "  'RCL',\n",
              "  'LEN',\n",
              "  'CPRT',\n",
              "  'PWR',\n",
              "  'STX',\n",
              "  'VRSN',\n",
              "  'CBRE',\n",
              "  'TYL',\n",
              "  'WDC',\n",
              "  'HUM',\n",
              "  'FCX'],\n",
              " ['ABC',\n",
              "  'TPR',\n",
              "  'MTCH',\n",
              "  'ALK',\n",
              "  'TJX',\n",
              "  'HUM',\n",
              "  'PKI',\n",
              "  'BKNG',\n",
              "  'WDC',\n",
              "  'AMZN',\n",
              "  'VTR',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'MAR',\n",
              "  'EQIX',\n",
              "  'TT',\n",
              "  'CVS',\n",
              "  'O',\n",
              "  'MCO',\n",
              "  'HPQ',\n",
              "  'TSN',\n",
              "  'AOS',\n",
              "  'AMD',\n",
              "  'GLW',\n",
              "  'CNC',\n",
              "  'AAPL',\n",
              "  'URI',\n",
              "  'VLO',\n",
              "  'MNST',\n",
              "  'INCY'],\n",
              " ['WYNN',\n",
              "  'FDS',\n",
              "  'PTC',\n",
              "  'MU',\n",
              "  'NVDA',\n",
              "  'SBAC',\n",
              "  'AKAM',\n",
              "  'GRMN',\n",
              "  'STLD',\n",
              "  'KMX',\n",
              "  'CI',\n",
              "  'BIIB',\n",
              "  'UNH',\n",
              "  'LRCX',\n",
              "  'FCX',\n",
              "  'TRMB',\n",
              "  'NEM',\n",
              "  'NFLX',\n",
              "  'MNST',\n",
              "  'ADSK',\n",
              "  'INCY',\n",
              "  'ATVI',\n",
              "  'VRSN',\n",
              "  'CTSH',\n",
              "  'PXD',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'EW',\n",
              "  'AMZN',\n",
              "  'ILMN'],\n",
              " ['WYNN',\n",
              "  'FDS',\n",
              "  'PTC',\n",
              "  'MU',\n",
              "  'NVDA',\n",
              "  'SBAC',\n",
              "  'AKAM',\n",
              "  'GRMN',\n",
              "  'STLD',\n",
              "  'KMX',\n",
              "  'CI',\n",
              "  'BIIB',\n",
              "  'UNH',\n",
              "  'LRCX',\n",
              "  'FCX',\n",
              "  'TRMB',\n",
              "  'NEM',\n",
              "  'NFLX',\n",
              "  'MNST',\n",
              "  'ADSK',\n",
              "  'INCY',\n",
              "  'ATVI',\n",
              "  'VRSN',\n",
              "  'CTSH',\n",
              "  'PXD',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'EW',\n",
              "  'AMZN',\n",
              "  'ILMN'],\n",
              " ['WYNN',\n",
              "  'FDS',\n",
              "  'PTC',\n",
              "  'MU',\n",
              "  'NVDA',\n",
              "  'SBAC',\n",
              "  'AKAM',\n",
              "  'GRMN',\n",
              "  'STLD',\n",
              "  'KMX',\n",
              "  'CI',\n",
              "  'BIIB',\n",
              "  'UNH',\n",
              "  'LRCX',\n",
              "  'FCX',\n",
              "  'TRMB',\n",
              "  'NEM',\n",
              "  'NFLX',\n",
              "  'MNST',\n",
              "  'ADSK',\n",
              "  'INCY',\n",
              "  'ATVI',\n",
              "  'VRSN',\n",
              "  'CTSH',\n",
              "  'PXD',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'EW',\n",
              "  'AMZN',\n",
              "  'ILMN'],\n",
              " ['WYNN',\n",
              "  'FDS',\n",
              "  'PTC',\n",
              "  'MU',\n",
              "  'NVDA',\n",
              "  'SBAC',\n",
              "  'AKAM',\n",
              "  'GRMN',\n",
              "  'STLD',\n",
              "  'KMX',\n",
              "  'CI',\n",
              "  'BIIB',\n",
              "  'UNH',\n",
              "  'LRCX',\n",
              "  'FCX',\n",
              "  'TRMB',\n",
              "  'NEM',\n",
              "  'NFLX',\n",
              "  'MNST',\n",
              "  'ADSK',\n",
              "  'INCY',\n",
              "  'ATVI',\n",
              "  'VRSN',\n",
              "  'CTSH',\n",
              "  'PXD',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'EW',\n",
              "  'AMZN',\n",
              "  'ILMN'],\n",
              " ['WYNN',\n",
              "  'FDS',\n",
              "  'PTC',\n",
              "  'MU',\n",
              "  'NVDA',\n",
              "  'SBAC',\n",
              "  'AKAM',\n",
              "  'GRMN',\n",
              "  'STLD',\n",
              "  'KMX',\n",
              "  'CI',\n",
              "  'BIIB',\n",
              "  'UNH',\n",
              "  'LRCX',\n",
              "  'FCX',\n",
              "  'TRMB',\n",
              "  'NEM',\n",
              "  'NFLX',\n",
              "  'MNST',\n",
              "  'ADSK',\n",
              "  'INCY',\n",
              "  'ATVI',\n",
              "  'VRSN',\n",
              "  'CTSH',\n",
              "  'PXD',\n",
              "  'TER',\n",
              "  'AAPL',\n",
              "  'EW',\n",
              "  'AMZN',\n",
              "  'ILMN'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['AMD',\n",
              "  'TER',\n",
              "  'EBAY',\n",
              "  'MGM',\n",
              "  'CNC',\n",
              "  'WYNN',\n",
              "  'MCO',\n",
              "  'NEM',\n",
              "  'SWKS',\n",
              "  'URI',\n",
              "  'MAS',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'ATVI',\n",
              "  'BG',\n",
              "  'MNST',\n",
              "  'HOLX',\n",
              "  'ADSK',\n",
              "  'TTWO',\n",
              "  'CTSH',\n",
              "  'INCY',\n",
              "  'SBAC',\n",
              "  'RCL',\n",
              "  'VRSN',\n",
              "  'ILMN',\n",
              "  'EQIX',\n",
              "  'NFLX',\n",
              "  'AAPL',\n",
              "  'ALK',\n",
              "  'AKAM'],\n",
              " ['INCY',\n",
              "  'VRTX',\n",
              "  'MAR',\n",
              "  'CRL',\n",
              "  'LNC',\n",
              "  'AAPL',\n",
              "  'SWKS',\n",
              "  'CRM',\n",
              "  'TSN',\n",
              "  'HOLX',\n",
              "  'CTSH',\n",
              "  'DHI',\n",
              "  'RMD',\n",
              "  'ADSK',\n",
              "  'BKNG',\n",
              "  'DLR',\n",
              "  'EQIX',\n",
              "  'LRCX',\n",
              "  'VLO',\n",
              "  'FCX',\n",
              "  'TPR',\n",
              "  'STX',\n",
              "  'SBAC',\n",
              "  'IDXX',\n",
              "  'CTRA',\n",
              "  'DXCM',\n",
              "  'AMD',\n",
              "  'AMZN',\n",
              "  'MPWR',\n",
              "  'ALK'],\n",
              " ['INCY',\n",
              "  'VRTX',\n",
              "  'MAR',\n",
              "  'CRL',\n",
              "  'LNC',\n",
              "  'AAPL',\n",
              "  'SWKS',\n",
              "  'CRM',\n",
              "  'TSN',\n",
              "  'HOLX',\n",
              "  'CTSH',\n",
              "  'DHI',\n",
              "  'RMD',\n",
              "  'ADSK',\n",
              "  'BKNG',\n",
              "  'DLR',\n",
              "  'EQIX',\n",
              "  'LRCX',\n",
              "  'VLO',\n",
              "  'FCX',\n",
              "  'TPR',\n",
              "  'STX',\n",
              "  'SBAC',\n",
              "  'IDXX',\n",
              "  'CTRA',\n",
              "  'DXCM',\n",
              "  'AMD',\n",
              "  'AMZN',\n",
              "  'MPWR',\n",
              "  'ALK']]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_stock_selection = pd.DataFrame(stock_selection)\n",
        "df_stock_selection.to_csv(\"drive/My Drive/FYP Data/stock_selection_rolling.csv\",index=True, header=True )"
      ],
      "metadata": {
        "id": "yDqTIxwYRvxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##fintune with past 4 years"
      ],
      "metadata": {
        "id": "ZBcKy-HZSQrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
        "mcp = ModelCheckpoint(filepath='/content/drive/My Drive/FYP Data/weights_GRU_rolling.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)\n",
        "predictions = []\n",
        "\n",
        "for i in tqdm(range(26)):\n",
        "  if i == 0:\n",
        "    GRU_model = keras.models.load_model('/content/drive/My Drive/FYP Data/weights_GRU.h5')\n",
        "  else:\n",
        "    GRU_model = keras.models.load_model('/content/drive/My Drive/FYP Data/weights_GRU_rolling.h5')\n",
        "  prediction = GRU_model.predict(x_test[i:i+1])\n",
        "  predictions.append(prediction)\n",
        "  x_train = X[train_point-4:train_point+i]\n",
        "  y_train = Y[train_point-4:train_point+i]\n",
        "\n",
        "  #fintune model\n",
        "  history = GRU_model.fit(x_train, y_train, shuffle=True, epochs=10, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b19673bd7f7a43cca8c8480ff9d72dc2",
            "db66ec605bf14d1aabf48b457425a9c7",
            "0148daf828a141539ac9b6f5bdd0869e",
            "e4e93352d4534a35b9294cb136f27dee",
            "48d1e82850a740bc8098e1f1dfbd5e93",
            "baf6cecf30db49e3ab4fb6e3a439d682",
            "125692f83a414fe8be9b8e2bfec81f21",
            "27c56b8b751c4da4aeafeb947a605e77",
            "3b4fa05dbacc4459be3ece8c3541f402",
            "dbe358eb35cd482a98a626041fef8ebb",
            "45564c76f9864c52b1e233d1c9783f18"
          ]
        },
        "id": "He2Pew60Rh3w",
        "outputId": "5bb5796c-7668-4f4f-d7ec-6a7e42e724ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/26 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b19673bd7f7a43cca8c8480ff9d72dc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2333\n",
            "Epoch 1: val_loss improved from inf to 0.19986, saving model to /content/drive/My Drive/FYP Data/weights_GRU_rolling.h5\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.2333 - val_loss: 0.1999 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2516\n",
            "Epoch 2: val_loss improved from 0.19986 to 0.19802, saving model to /content/drive/My Drive/FYP Data/weights_GRU_rolling.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2516 - val_loss: 0.1980 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2382\n",
            "Epoch 3: val_loss did not improve from 0.19802\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.2382 - val_loss: 0.1982 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2153\n",
            "Epoch 4: val_loss did not improve from 0.19802\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2153 - val_loss: 0.2046 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1976\n",
            "Epoch 5: val_loss did not improve from 0.19802\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1976 - val_loss: 0.2131 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1964\n",
            "Epoch 6: val_loss did not improve from 0.19802\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1964 - val_loss: 0.2240 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1950\n",
            "Epoch 7: val_loss did not improve from 0.19802\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1950 - val_loss: 0.2388 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1821\n",
            "Epoch 8: val_loss did not improve from 0.19802\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1821 - val_loss: 0.2580 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1766\n",
            "Epoch 9: val_loss did not improve from 0.19802\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1766 - val_loss: 0.2755 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1651\n",
            "Epoch 10: val_loss did not improve from 0.19802\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1651 - val_loss: 0.2910 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2276\n",
            "Epoch 1: val_loss improved from 0.19802 to 0.19442, saving model to /content/drive/My Drive/FYP Data/weights_GRU_rolling.h5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2276 - val_loss: 0.1944 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2165\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2165 - val_loss: 0.1974 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2058\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2058 - val_loss: 0.2024 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2152\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2152 - val_loss: 0.2071 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2171\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2171 - val_loss: 0.2126 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1996\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1996 - val_loss: 0.2224 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1926\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.1926 - val_loss: 0.2295 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1786\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1786 - val_loss: 0.2302 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1987\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1987 - val_loss: 0.2299 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1837\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1837 - val_loss: 0.2307 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2196\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2196 - val_loss: 0.1993 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2024\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.2024 - val_loss: 0.2051 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2096\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2096 - val_loss: 0.2103 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2078\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.2078 - val_loss: 0.2160 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1941\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1941 - val_loss: 0.2249 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1874\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1874 - val_loss: 0.2331 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1866\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1866 - val_loss: 0.2362 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1865\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1865 - val_loss: 0.2413 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1906\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.1906 - val_loss: 0.2478 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2084\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2084 - val_loss: 0.2503 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2310\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2310 - val_loss: 0.1993 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2201\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2201 - val_loss: 0.2015 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2141\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2141 - val_loss: 0.2032 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2089\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2089 - val_loss: 0.2034 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1973\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1973 - val_loss: 0.2037 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2042\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2042 - val_loss: 0.2046 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2038\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2038 - val_loss: 0.2064 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1862\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1862 - val_loss: 0.2073 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1994\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1994 - val_loss: 0.2084 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1994\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1994 - val_loss: 0.2083 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2279\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2279 - val_loss: 0.2052 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2272\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2272 - val_loss: 0.2072 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2093\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.2093 - val_loss: 0.2099 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2041\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.2041 - val_loss: 0.2140 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2074\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2074 - val_loss: 0.2183 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2092\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2092 - val_loss: 0.2200 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2011\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2011 - val_loss: 0.2209 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2179\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2179 - val_loss: 0.2218 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2001\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2001 - val_loss: 0.2256 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1980\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1980 - val_loss: 0.2266 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2312\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2312 - val_loss: 0.2057 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2199\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2199 - val_loss: 0.2125 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2223\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2223 - val_loss: 0.2186 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2196\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2196 - val_loss: 0.2257 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2095\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2095 - val_loss: 0.2342 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2048\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2048 - val_loss: 0.2429 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2049\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2049 - val_loss: 0.2498 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1999\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1999 - val_loss: 0.2558 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1956\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1956 - val_loss: 0.2616 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1908\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1908 - val_loss: 0.2648 - lr: 0.0100\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2284\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2284 - val_loss: 0.2016 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2294\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2294 - val_loss: 0.2041 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2229\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2229 - val_loss: 0.2057 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2193\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2193 - val_loss: 0.2080 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2149\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2149 - val_loss: 0.2104 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2240\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2240 - val_loss: 0.2130 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2234\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2234 - val_loss: 0.2145 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2149\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2149 - val_loss: 0.2177 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2108\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2108 - val_loss: 0.2228 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2054\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2054 - val_loss: 0.2236 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2275\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2275 - val_loss: 0.1990 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2221\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2221 - val_loss: 0.2042 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2253\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2253 - val_loss: 0.2081 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2255\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2255 - val_loss: 0.2082 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2209\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2209 - val_loss: 0.2107 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2196\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2196 - val_loss: 0.2160 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2188\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.2188 - val_loss: 0.2202 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2112\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2112 - val_loss: 0.2207 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2100\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2100 - val_loss: 0.2223 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2145\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2145 - val_loss: 0.2263 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2281\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2281 - val_loss: 0.1970 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2305\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2305 - val_loss: 0.2013 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2300\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2300 - val_loss: 0.2026 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2273\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2273 - val_loss: 0.2047 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2261\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2261 - val_loss: 0.2094 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2111\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2111 - val_loss: 0.2142 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2179\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2179 - val_loss: 0.2170 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2109\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2109 - val_loss: 0.2184 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2077\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2077 - val_loss: 0.2208 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1986\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1986 - val_loss: 0.2229 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2315\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2315 - val_loss: 0.1964 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2285\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2285 - val_loss: 0.1996 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2314\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2314 - val_loss: 0.2015 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2216\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2216 - val_loss: 0.2043 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2207\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2207 - val_loss: 0.2082 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2238\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2238 - val_loss: 0.2121 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2121\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2121 - val_loss: 0.2152 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2188\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2188 - val_loss: 0.2189 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2085\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2085 - val_loss: 0.2217 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2071\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2071 - val_loss: 0.2215 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 3s - loss: 0.2300\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 4s 1s/step - loss: 0.2295 - val_loss: 0.2039 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2371\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2348 - val_loss: 0.2045 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2289\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2269 - val_loss: 0.2166 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2280\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2299 - val_loss: 0.2170 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2320\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2325 - val_loss: 0.2253 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2306\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2278 - val_loss: 0.2260 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2256\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2308 - val_loss: 0.2177 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2262\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2260 - val_loss: 0.2113 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2346\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2291 - val_loss: 0.2147 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2347\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2375 - val_loss: 0.2206 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 4s - loss: 0.2345\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 6s 1s/step - loss: 0.2331 - val_loss: 0.2015 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2339\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2322 - val_loss: 0.2010 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2293\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2287 - val_loss: 0.2009 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2270\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2211 - val_loss: 0.1998 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2282\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2315 - val_loss: 0.1986 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2359\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2315 - val_loss: 0.2005 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2178\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2237 - val_loss: 0.2029 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2173\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2180 - val_loss: 0.2022 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2115\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2143 - val_loss: 0.2005 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2135\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2139 - val_loss: 0.1986 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 3s - loss: 0.2304\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 4s 1s/step - loss: 0.2327 - val_loss: 0.1979 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2302\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2318 - val_loss: 0.2004 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2187\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2181 - val_loss: 0.2017 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2198\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2225 - val_loss: 0.2033 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2276\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2272 - val_loss: 0.2012 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2282\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2256 - val_loss: 0.2025 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2143\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2186 - val_loss: 0.1964 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2188\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2156 - val_loss: 0.1974 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2136\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2142 - val_loss: 0.1984 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2157\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2167 - val_loss: 0.2007 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 3s - loss: 0.2325\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 4s 1s/step - loss: 0.2317 - val_loss: 0.2008 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2342\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.2345 - val_loss: 0.2103 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2319\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2313 - val_loss: 0.2127 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2244\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2250 - val_loss: 0.2121 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2180\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2183 - val_loss: 0.2104 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2154\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2181 - val_loss: 0.2108 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2151\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2209 - val_loss: 0.2116 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2257\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2238 - val_loss: 0.2087 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2146\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2144 - val_loss: 0.2072 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2143\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.2216 - val_loss: 0.2076 - lr: 0.0100\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 3s - loss: 0.2341\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 5s 1s/step - loss: 0.2324 - val_loss: 0.2078 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2373\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2310 - val_loss: 0.2159 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2277\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2297 - val_loss: 0.2198 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2332\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.2301 - val_loss: 0.2238 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2200\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2215 - val_loss: 0.2187 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2213\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.2216 - val_loss: 0.2176 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2158\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.2190 - val_loss: 0.2134 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2066\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.2065 - val_loss: 0.2095 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2115\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2170 - val_loss: 0.2095 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2137\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2099 - val_loss: 0.2072 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 3s - loss: 0.2358\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 4s 1s/step - loss: 0.2362 - val_loss: 0.2064 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2225\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2278 - val_loss: 0.2132 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2351\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2295 - val_loss: 0.2159 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2242\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2214 - val_loss: 0.2181 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2175\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2182 - val_loss: 0.2173 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2161\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2135 - val_loss: 0.2137 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2170\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2163 - val_loss: 0.2122 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2220\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2195 - val_loss: 0.2097 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2082\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2123 - val_loss: 0.2082 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2118\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2114 - val_loss: 0.2076 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 3s - loss: 0.2319\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 4s 1s/step - loss: 0.2375 - val_loss: 0.2105 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2398\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2350 - val_loss: 0.2145 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2296\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.2327 - val_loss: 0.2179 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2206\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2210 - val_loss: 0.2218 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2225\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2183 - val_loss: 0.2221 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2203\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.2187 - val_loss: 0.2216 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2137\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2172 - val_loss: 0.2185 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2090\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2140 - val_loss: 0.2151 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2046\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2055 - val_loss: 0.2119 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2091\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2085 - val_loss: 0.2099 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 3s - loss: 0.2332\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 5s 1s/step - loss: 0.2336 - val_loss: 0.2124 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2295\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2309 - val_loss: 0.2164 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2314\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2283 - val_loss: 0.2208 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2314\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2259 - val_loss: 0.2243 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2187\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2209 - val_loss: 0.2250 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2120\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2176 - val_loss: 0.2233 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2184\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2171 - val_loss: 0.2212 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2147\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2110 - val_loss: 0.2192 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2172\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2199 - val_loss: 0.2216 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2265\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2193 - val_loss: 0.2154 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 3s - loss: 0.2295\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 4s 1s/step - loss: 0.2307 - val_loss: 0.2138 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2337\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2357 - val_loss: 0.2119 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2211\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2260 - val_loss: 0.2143 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2153\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2217 - val_loss: 0.2123 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2277\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2290 - val_loss: 0.2142 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2088\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2133 - val_loss: 0.2098 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2245\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2224 - val_loss: 0.2081 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2189\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2202 - val_loss: 0.2071 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2137\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2135 - val_loss: 0.2065 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2100\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2156 - val_loss: 0.2070 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 4s - loss: 0.2354\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 6s 1s/step - loss: 0.2339 - val_loss: 0.2119 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2305\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2330 - val_loss: 0.2121 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2365\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2329 - val_loss: 0.2116 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2242\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2255 - val_loss: 0.2101 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2249\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2214 - val_loss: 0.2092 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2217\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2198 - val_loss: 0.2083 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2139\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2200 - val_loss: 0.2083 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2200\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2224 - val_loss: 0.2077 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2192\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2192 - val_loss: 0.2067 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2127\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2149 - val_loss: 0.2046 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 3s - loss: 0.2349\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 4s 1s/step - loss: 0.2355 - val_loss: 0.2052 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2422\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2358 - val_loss: 0.2065 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2270\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2313 - val_loss: 0.2074 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2175\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2262 - val_loss: 0.2094 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2295\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.2232 - val_loss: 0.2100 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2323\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2237 - val_loss: 0.2084 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2230\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.2211 - val_loss: 0.2079 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2164\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2183 - val_loss: 0.2074 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2278\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2204 - val_loss: 0.2056 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2188\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.2177 - val_loss: 0.2047 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 2s - loss: 0.2297\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 4s 1s/step - loss: 0.2322 - val_loss: 0.2076 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2358\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2347 - val_loss: 0.2073 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2249\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2219 - val_loss: 0.2051 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2232\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2298 - val_loss: 0.2045 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2262\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2224 - val_loss: 0.2034 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2189\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2169 - val_loss: 0.2048 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2262\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2211 - val_loss: 0.2006 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2191\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2172 - val_loss: 0.1996 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2067\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2088 - val_loss: 0.1994 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2193\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2130 - val_loss: 0.1997 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 3s - loss: 0.2315\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 5s 1s/step - loss: 0.2352 - val_loss: 0.2091 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2333\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.2346 - val_loss: 0.2098 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2228\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.2277 - val_loss: 0.2082 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2349\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2319 - val_loss: 0.2065 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2280\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2242 - val_loss: 0.2061 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2212\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2231 - val_loss: 0.2047 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2187\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.2187 - val_loss: 0.2036 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2189\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2184 - val_loss: 0.2029 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2144\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2149 - val_loss: 0.2032 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2185\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2178 - val_loss: 0.2030 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/3 [=========>....................] - ETA: 6s - loss: 0.2275\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 4s 598ms/step - loss: 0.2334 - val_loss: 0.2083 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2351\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2379 - val_loss: 0.2074 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2282\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2317 - val_loss: 0.2127 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2359\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2370 - val_loss: 0.2185 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2403\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2452 - val_loss: 0.2083 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2311\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2347 - val_loss: 0.2074 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2261\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2323 - val_loss: 0.2114 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2284\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2340 - val_loss: 0.2160 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2504\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2451 - val_loss: 0.2126 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2417\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2516 - val_loss: 0.2144 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/10\n",
            "1/3 [=========>....................] - ETA: 6s - loss: 0.2342\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 4s 585ms/step - loss: 0.2321 - val_loss: 0.2054 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2426\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2413 - val_loss: 0.2074 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2282\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2314 - val_loss: 0.2114 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2334\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2298 - val_loss: 0.2037 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2337\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2323 - val_loss: 0.2011 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2221\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2254 - val_loss: 0.2018 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2260\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2284 - val_loss: 0.2007 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2313\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2273 - val_loss: 0.1969 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2223\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2251 - val_loss: 0.2025 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2251\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2248 - val_loss: 0.2069 - lr: 0.0100\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Epoch 1/10\n",
            "1/3 [=========>....................] - ETA: 6s - loss: 0.2365\n",
            "Epoch 1: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 5s 617ms/step - loss: 0.2362 - val_loss: 0.2036 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2421\n",
            "Epoch 2: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2369 - val_loss: 0.2048 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2374\n",
            "Epoch 3: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2302 - val_loss: 0.2010 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2275\n",
            "Epoch 4: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2257 - val_loss: 0.1990 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2222\n",
            "Epoch 5: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2248 - val_loss: 0.1997 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2284\n",
            "Epoch 6: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2247 - val_loss: 0.1965 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2246\n",
            "Epoch 7: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2197 - val_loss: 0.1976 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2294\n",
            "Epoch 8: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2274 - val_loss: 0.1955 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2124\n",
            "Epoch 9: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2190 - val_loss: 0.1974 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2170\n",
            "Epoch 10: val_loss did not improve from 0.19442\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2245 - val_loss: 0.1964 - lr: 0.0100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "predictions = np.vstack([predictions[i] for i in range(26)])\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
        "print('rmse', rmse)\n",
        "\n",
        "#profit test\n",
        "test_period = time[n_past+train_point:]\n",
        "\n",
        "#calculate the top 10 stock we picked\n",
        "stock_selection_index = np.argsort(predictions)[:,-30:]\n",
        "stock_selection = []\n",
        "for index in stock_selection_index:\n",
        "  stock_list = []\n",
        "  for stock_i in index:\n",
        "    stock_list.append(SP500_ticker[stock_i])\n",
        "  stock_selection.append(stock_list)\n",
        "\n",
        "#trading simulate\n",
        "original_money = 0\n",
        "trading = 0\n",
        "for stock in stock_selection[0]:\n",
        "  original_money = original_money + stock_data[stock][\"Close\"][test_period[0]]\n",
        "  #print(stock_data[stock][\"Close\"][test_period[0]])\n",
        "\n",
        "\n",
        "trading = - original_money\n",
        "i = 0\n",
        "\n",
        "for stock_list in stock_selection:\n",
        "  if i==0 or 25:\n",
        "    i = i + 1\n",
        "    continue\n",
        "\n",
        "  for stock in stock_list:\n",
        "    trading = trading + stock_data[stock][\"Close\"][test_period[i-1]] - stock_data[stock][\"Close\"][test_period[i]]\n",
        "    #print(trading)\n",
        "  \n",
        "  i = i + 1\n",
        "\n",
        "end_money = 0\n",
        "\n",
        "for stock in stock_selection[-1]:\n",
        "  trading = trading + stock_data[stock][\"Close\"][test_period[-1]]\n",
        "  end_money = end_money + stock_data[stock][\"Close\"][test_period[-1]]\n",
        "  #print(stock_data[stock][\"Close\"][test_period[-1]])\n",
        "\n",
        "print('The money earned by LSTM with Fundamental and Technical data is ',trading)\n",
        "print('The money earned by LSTM with Fundamental and Technical data is %',trading*100/original_money)\n",
        "print('The money earned by LSTM with Fundamental and Technical data is %(unchange)',(end_money-original_money)*100/original_money)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J9Nrn0PSmZN",
        "outputId": "1c622911-e588-4b61-851a-bddf045c1626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse 0.0207318021662784\n",
            "The money earned by LSTM with Fundamental and Technical data is  -782.1986141204834\n",
            "The money earned by LSTM with Fundamental and Technical data is % -10.99240659100257\n",
            "The money earned by LSTM with Fundamental and Technical data is %(unchange) -10.99240659100257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stock_selection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IPVNaugSpET",
        "outputId": "0ab2b606-f13a-462a-d0cd-f00a3e2bc0e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['AVB',\n",
              "  'CNC',\n",
              "  'NSC',\n",
              "  'CI',\n",
              "  'VTR',\n",
              "  'MO',\n",
              "  'ISRG',\n",
              "  'CTSH',\n",
              "  'AMZN',\n",
              "  'EXR',\n",
              "  'SPG',\n",
              "  'ALGN',\n",
              "  'TDG',\n",
              "  'ROST',\n",
              "  'JNPR',\n",
              "  'NFLX',\n",
              "  'DHI',\n",
              "  'HUM',\n",
              "  'ODFL',\n",
              "  'ALK',\n",
              "  'TER',\n",
              "  'EW',\n",
              "  'TJX',\n",
              "  'PWR',\n",
              "  'FFIV',\n",
              "  'BKNG',\n",
              "  'INCY',\n",
              "  'ILMN',\n",
              "  'URI',\n",
              "  'AAPL'],\n",
              " ['SPG',\n",
              "  'DGX',\n",
              "  'CPRT',\n",
              "  'JNPR',\n",
              "  'BALL',\n",
              "  'AMZN',\n",
              "  'A',\n",
              "  'AES',\n",
              "  'AKAM',\n",
              "  'TER',\n",
              "  'NSC',\n",
              "  'AVB',\n",
              "  'FCX',\n",
              "  'RMD',\n",
              "  'CPT',\n",
              "  'ODFL',\n",
              "  'TPR',\n",
              "  'BIO',\n",
              "  'EW',\n",
              "  'ALK',\n",
              "  'GOOGL',\n",
              "  'FFIV',\n",
              "  'EXR',\n",
              "  'BKNG',\n",
              "  'PWR',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'AAPL',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG'],\n",
              " ['CAG',\n",
              "  'HOLX',\n",
              "  'AMD',\n",
              "  'GOOG',\n",
              "  'FFIV',\n",
              "  'ENPH',\n",
              "  'ODFL',\n",
              "  'MPWR',\n",
              "  'GOOGL',\n",
              "  'MGM',\n",
              "  'RMD',\n",
              "  'BALL',\n",
              "  'CPRT',\n",
              "  'NVDA',\n",
              "  'NSC',\n",
              "  'TJX',\n",
              "  'DHI',\n",
              "  'EW',\n",
              "  'BKNG',\n",
              "  'FCX',\n",
              "  'EXR',\n",
              "  'SBUX',\n",
              "  'TPR',\n",
              "  'PWR',\n",
              "  'ALK',\n",
              "  'AAPL',\n",
              "  'AES',\n",
              "  'BIO',\n",
              "  'ALGN',\n",
              "  'TDG']]"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_stock_selection = pd.DataFrame(stock_selection)\n",
        "df_stock_selection.to_csv(\"drive/My Drive/FYP Data/stock_selection_rolling_4.csv\",index=True, header=True )"
      ],
      "metadata": {
        "id": "_ds5fVhZSy5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
        "mcp = ModelCheckpoint(filepath='/content/drive/My Drive/FYP Data/weights_LSTM_rolling.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)\n",
        "predictions = []\n",
        "\n",
        "for i in tqdm(range(26)):\n",
        "  if i == 0:\n",
        "    LSTM_model = keras.models.load_model('/content/drive/My Drive/FYP Data/weights_w_fundamental_1.h5')\n",
        "  else:\n",
        "    LSTM_model = keras.models.load_model('/content/drive/My Drive/FYP Data/weights_LSTM_rolling.h5')\n",
        "  prediction = LSTM_model.predict(x_test[i:i+1])\n",
        "  predictions.append(prediction)\n",
        "  x_train = X[train_point-4:train_point+i]\n",
        "  y_train = Y[train_point-4:train_point+i]\n",
        "\n",
        "  #fintune model\n",
        "  history = LSTM_model.fit(x_train, y_train, shuffle=True, epochs=10, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "10616f8434b94d13bab1807481625117",
            "b8a93a35bbf44834a56aa0dac6e77097",
            "2d884b6b89934f0696536297f17c2cef",
            "45de9013b68647988f70f78833b96e90",
            "ca2cb7a16e5c455aaef2d7c49683a03e",
            "157cb1810f694208bcfaa14f5228ddea",
            "b5d73444c3f04f6589677dc5595bec52",
            "4bd95a16eefe45d3beb86601a403bff4",
            "f4a5ecbde6074d36b358907b01befdb6",
            "b4b6e8c3ac2e46499966fdd253687ca4",
            "b17d573f93a14564abe4f280a50eb892"
          ]
        },
        "id": "uKtAlNHNV9GY",
        "outputId": "b7ebc345-5bf1-4fea-a6a4-cb30945517ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/26 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10616f8434b94d13bab1807481625117"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 649ms/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3160\n",
            "Epoch 1: val_loss improved from inf to 0.20345, saving model to /content/drive/My Drive/FYP Data/weights_LSTM_rolling.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3160 - val_loss: 0.2035 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2828\n",
            "Epoch 2: val_loss did not improve from 0.20345\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2828 - val_loss: 0.2126 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2998\n",
            "Epoch 3: val_loss did not improve from 0.20345\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2998 - val_loss: 0.2199 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2648\n",
            "Epoch 4: val_loss did not improve from 0.20345\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2648 - val_loss: 0.2312 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2725\n",
            "Epoch 5: val_loss did not improve from 0.20345\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2725 - val_loss: 0.2460 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2498\n",
            "Epoch 6: val_loss did not improve from 0.20345\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2498 - val_loss: 0.2660 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2597\n",
            "Epoch 7: val_loss did not improve from 0.20345\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2597 - val_loss: 0.2837 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2533\n",
            "Epoch 8: val_loss did not improve from 0.20345\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2533 - val_loss: 0.3080 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2643\n",
            "Epoch 9: val_loss did not improve from 0.20345\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2643 - val_loss: 0.3189 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2747\n",
            "Epoch 10: val_loss did not improve from 0.20345\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2747 - val_loss: 0.3128 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 646ms/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2947\n",
            "Epoch 1: val_loss improved from 0.20345 to 0.19880, saving model to /content/drive/My Drive/FYP Data/weights_LSTM_rolling.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2947 - val_loss: 0.1988 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3098\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3098 - val_loss: 0.2049 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2726\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2726 - val_loss: 0.2202 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2812\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2812 - val_loss: 0.2367 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2937\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2937 - val_loss: 0.2408 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2966\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2966 - val_loss: 0.2375 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2744\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2744 - val_loss: 0.2416 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2732\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2732 - val_loss: 0.2453 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2660\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2660 - val_loss: 0.2431 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2611\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2611 - val_loss: 0.2398 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 638ms/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2904\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2904 - val_loss: 0.2074 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2825\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2825 - val_loss: 0.2222 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2898\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2898 - val_loss: 0.2291 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2820\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2820 - val_loss: 0.2339 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2779\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2779 - val_loss: 0.2377 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2564\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2564 - val_loss: 0.2458 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2665\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.2665 - val_loss: 0.2516 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2849\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2849 - val_loss: 0.2463 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2436\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2436 - val_loss: 0.2431 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2483\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2483 - val_loss: 0.2412 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 730ms/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3005\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3005 - val_loss: 0.2000 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2877\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2877 - val_loss: 0.2030 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2730\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2730 - val_loss: 0.2076 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2746\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2746 - val_loss: 0.2129 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2739\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2739 - val_loss: 0.2140 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2695\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2695 - val_loss: 0.2117 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2693\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2693 - val_loss: 0.2129 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2661\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2661 - val_loss: 0.2147 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2637\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2637 - val_loss: 0.2122 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2652\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2652 - val_loss: 0.2095 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 646ms/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2952\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2952 - val_loss: 0.2056 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2902\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2902 - val_loss: 0.2130 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2831\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2831 - val_loss: 0.2223 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2917\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2917 - val_loss: 0.2252 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2796\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2796 - val_loss: 0.2219 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2863\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2863 - val_loss: 0.2216 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2640\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2640 - val_loss: 0.2276 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2770\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2770 - val_loss: 0.2331 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2714\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2714 - val_loss: 0.2380 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2776\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2776 - val_loss: 0.2372 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 647ms/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2855\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2855 - val_loss: 0.2134 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2884\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2884 - val_loss: 0.2246 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2845\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2845 - val_loss: 0.2356 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2843\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2843 - val_loss: 0.2438 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2749\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2749 - val_loss: 0.2498 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2747\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2747 - val_loss: 0.2555 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2835\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2835 - val_loss: 0.2623 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2732\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2732 - val_loss: 0.2660 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2523\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2523 - val_loss: 0.2668 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2617\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2617 - val_loss: 0.2664 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 637ms/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3018\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3018 - val_loss: 0.2132 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3050\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3050 - val_loss: 0.2226 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3090\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3090 - val_loss: 0.2234 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2817\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2817 - val_loss: 0.2241 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2847\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2847 - val_loss: 0.2238 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2838\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2838 - val_loss: 0.2254 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2966\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2966 - val_loss: 0.2312 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2789\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2789 - val_loss: 0.2336 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2866\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2866 - val_loss: 0.2328 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2793\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2793 - val_loss: 0.2296 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 635ms/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2925\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2925 - val_loss: 0.2112 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3034\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.3034 - val_loss: 0.2199 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2920\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2920 - val_loss: 0.2211 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2864\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2864 - val_loss: 0.2210 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2756\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2756 - val_loss: 0.2228 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2686\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2686 - val_loss: 0.2263 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2954\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2954 - val_loss: 0.2267 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2764\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2764 - val_loss: 0.2284 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2661\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2661 - val_loss: 0.2277 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2985\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2985 - val_loss: 0.2255 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 687ms/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3001\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3001 - val_loss: 0.2093 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2972\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2972 - val_loss: 0.2122 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2982\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2982 - val_loss: 0.2178 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3068\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3068 - val_loss: 0.2226 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3031\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3031 - val_loss: 0.2246 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2793\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2793 - val_loss: 0.2253 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3075\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3075 - val_loss: 0.2214 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2822\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2822 - val_loss: 0.2176 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2712\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2712 - val_loss: 0.2165 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2735\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2735 - val_loss: 0.2168 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 641ms/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2960\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2960 - val_loss: 0.2105 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2863\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2863 - val_loss: 0.2141 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3074\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3074 - val_loss: 0.2182 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3110\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3110 - val_loss: 0.2201 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3128\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.3128 - val_loss: 0.2149 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2895\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2895 - val_loss: 0.2125 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2781\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2781 - val_loss: 0.2149 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2873\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2873 - val_loss: 0.2188 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2942\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2942 - val_loss: 0.2152 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2843\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2843 - val_loss: 0.2133 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 651ms/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 1s - loss: 0.2933\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 2s 728ms/step - loss: 0.2972 - val_loss: 0.2139 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3070\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.3065 - val_loss: 0.2246 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3098\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.3121 - val_loss: 0.2299 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3183\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.3195 - val_loss: 0.2587 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3291\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.3247 - val_loss: 0.2496 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3067\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.3154 - val_loss: 0.2469 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3324\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.3275 - val_loss: 0.2592 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3348\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.3344 - val_loss: 0.2474 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3448\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.3400 - val_loss: 0.2416 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3364\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.3403 - val_loss: 0.2445 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 655ms/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 1s - loss: 0.3028\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 2s 739ms/step - loss: 0.3136 - val_loss: 0.2080 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3086\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.3008 - val_loss: 0.2106 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2845\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.2878 - val_loss: 0.2112 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2931\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.2979 - val_loss: 0.2072 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2976\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.2951 - val_loss: 0.2138 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3109\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.3161 - val_loss: 0.2047 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2831\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2871 - val_loss: 0.2280 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3190\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.3093 - val_loss: 0.2348 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3074\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.3109 - val_loss: 0.2283 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3108\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.3099 - val_loss: 0.2411 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 627ms/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 1s - loss: 0.2915\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.2913 - val_loss: 0.2084 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2943\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.2950 - val_loss: 0.2082 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3060\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.3093 - val_loss: 0.2163 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3094\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.3108 - val_loss: 0.2119 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2943\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2965 - val_loss: 0.2081 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2964\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.2905 - val_loss: 0.2235 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3177\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.3098 - val_loss: 0.2223 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2988\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2991 - val_loss: 0.2219 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2998\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.3109 - val_loss: 0.2138 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3037\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2997 - val_loss: 0.2142 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 682ms/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 1s - loss: 0.2962\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 2s 745ms/step - loss: 0.2951 - val_loss: 0.2116 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2912\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2928 - val_loss: 0.2172 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2917\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2930 - val_loss: 0.2134 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3061\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.3043 - val_loss: 0.2150 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2880\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2839 - val_loss: 0.2151 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2803\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2829 - val_loss: 0.2213 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2863\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2851 - val_loss: 0.2170 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2906\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.2875 - val_loss: 0.2230 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2877\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.2829 - val_loss: 0.2215 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2802\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2830 - val_loss: 0.2207 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 641ms/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 1s - loss: 0.2920\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 2s 741ms/step - loss: 0.3013 - val_loss: 0.2146 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3020\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2979 - val_loss: 0.2220 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2941\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2958 - val_loss: 0.2240 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2775\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2828 - val_loss: 0.2209 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2751\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2785 - val_loss: 0.2166 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2910\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2881 - val_loss: 0.2172 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2755\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2764 - val_loss: 0.2209 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2948\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2877 - val_loss: 0.2254 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2759\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2760 - val_loss: 0.2254 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2834\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2762 - val_loss: 0.2248 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 643ms/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 1s - loss: 0.3079\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 2s 737ms/step - loss: 0.3071 - val_loss: 0.2150 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2997\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.3004 - val_loss: 0.2207 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2915\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2939 - val_loss: 0.2266 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2975\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.3020 - val_loss: 0.2225 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2942\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2930 - val_loss: 0.2208 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2894\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.2895 - val_loss: 0.2149 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2795\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2796 - val_loss: 0.2161 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2772\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2847 - val_loss: 0.2217 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2876\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.2815 - val_loss: 0.2193 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2763\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.2807 - val_loss: 0.2188 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 639ms/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 1s - loss: 0.3048\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 2s 738ms/step - loss: 0.3077 - val_loss: 0.2142 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2982\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.3013 - val_loss: 0.2217 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2981\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2938 - val_loss: 0.2252 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2940\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2982 - val_loss: 0.2275 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2795\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2842 - val_loss: 0.2278 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2941\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2935 - val_loss: 0.2215 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2846\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2808 - val_loss: 0.2203 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2768\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2757 - val_loss: 0.2208 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2826\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2781 - val_loss: 0.2168 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2694\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2782 - val_loss: 0.2165 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 631ms/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 2s - loss: 0.2954\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 4s 790ms/step - loss: 0.2986 - val_loss: 0.2184 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2918\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2919 - val_loss: 0.2277 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3022\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.3029 - val_loss: 0.2307 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2821\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2852 - val_loss: 0.2303 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2809\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2779 - val_loss: 0.2293 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2756\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.2820 - val_loss: 0.2268 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2844\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2863 - val_loss: 0.2249 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2863\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.2857 - val_loss: 0.2244 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2905\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2910 - val_loss: 0.2242 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2737\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2758 - val_loss: 0.2214 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 660ms/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 1s - loss: 0.3032\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 2s 758ms/step - loss: 0.2988 - val_loss: 0.2199 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3084\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.3054 - val_loss: 0.2203 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2951\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2908 - val_loss: 0.2225 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2835\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2864 - val_loss: 0.2213 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2901\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2924 - val_loss: 0.2193 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2807\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2806 - val_loss: 0.2156 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2875\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2757 - val_loss: 0.2157 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2922\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2945 - val_loss: 0.2167 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2806\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2846 - val_loss: 0.2187 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2760\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2785 - val_loss: 0.2114 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 659ms/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 1s - loss: 0.3089\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 2s 737ms/step - loss: 0.3140 - val_loss: 0.2191 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3013\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2973 - val_loss: 0.2213 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3007\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2937 - val_loss: 0.2157 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2793\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2826 - val_loss: 0.2139 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2737\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2756 - val_loss: 0.2128 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2892\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2852 - val_loss: 0.2102 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2795\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.2906 - val_loss: 0.2166 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2883\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2816 - val_loss: 0.2122 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2964\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2919 - val_loss: 0.2198 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2876\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2868 - val_loss: 0.2127 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 628ms/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 1s - loss: 0.2939\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 2s 768ms/step - loss: 0.3004 - val_loss: 0.2175 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2962\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.3012 - val_loss: 0.2192 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2973\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2980 - val_loss: 0.2171 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2862\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2865 - val_loss: 0.2218 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2897\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2941 - val_loss: 0.2127 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2904\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2905 - val_loss: 0.2078 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2774\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2795 - val_loss: 0.2071 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2895\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2862 - val_loss: 0.2093 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2845\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2807 - val_loss: 0.2166 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2857\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2870 - val_loss: 0.2136 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 632ms/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 1s - loss: 0.3071\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 2s 733ms/step - loss: 0.3056 - val_loss: 0.2174 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3134\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.3072 - val_loss: 0.2188 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2850\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2877 - val_loss: 0.2126 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2982\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2953 - val_loss: 0.2147 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2945\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2919 - val_loss: 0.2047 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2785\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.2811 - val_loss: 0.2015 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2925\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.2938 - val_loss: 0.2030 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2756\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2824 - val_loss: 0.2061 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2737\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2748 - val_loss: 0.2070 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2892\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.2793 - val_loss: 0.2056 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 639ms/step\n",
            "Epoch 1/10\n",
            "1/2 [==============>...............] - ETA: 2s - loss: 0.3085\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 3s 794ms/step - loss: 0.3030 - val_loss: 0.2206 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2947\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.3088 - val_loss: 0.2209 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2872\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2883 - val_loss: 0.2164 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2778\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2939 - val_loss: 0.2154 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2916\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2865 - val_loss: 0.2085 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2844\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.2898 - val_loss: 0.2032 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2783\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2807 - val_loss: 0.2056 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2919\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.2917 - val_loss: 0.2059 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2811\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2787 - val_loss: 0.2123 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2827\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2804 - val_loss: 0.2125 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 691ms/step\n",
            "Epoch 1/10\n",
            "1/3 [=========>....................] - ETA: 3s - loss: 0.3065\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 3s 385ms/step - loss: 0.3032 - val_loss: 0.2214 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3090\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3080 - val_loss: 0.2190 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2925\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2968 - val_loss: 0.2245 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3016\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.3080 - val_loss: 0.2206 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3061\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.3060 - val_loss: 0.2231 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3093\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3082 - val_loss: 0.2234 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2886\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2975 - val_loss: 0.2210 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3060\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.3037 - val_loss: 0.2183 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3013\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.3072 - val_loss: 0.2228 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3199\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.3210 - val_loss: 0.2207 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 642ms/step\n",
            "Epoch 1/10\n",
            "1/3 [=========>....................] - ETA: 3s - loss: 0.2982\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 2s 377ms/step - loss: 0.3051 - val_loss: 0.2213 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3072\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3070 - val_loss: 0.2173 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2985\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3043 - val_loss: 0.2143 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2992\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.3032 - val_loss: 0.2106 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2940\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3023 - val_loss: 0.2264 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3165\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3162 - val_loss: 0.2141 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2969\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2980 - val_loss: 0.2195 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2972\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3007 - val_loss: 0.2292 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3158\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3083 - val_loss: 0.2185 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3053\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3042 - val_loss: 0.2354 - lr: 0.0100\n",
            "1/1 [==============================] - 1s 657ms/step\n",
            "Epoch 1/10\n",
            "1/3 [=========>....................] - ETA: 3s - loss: 0.3051\n",
            "Epoch 1: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 2s 378ms/step - loss: 0.3129 - val_loss: 0.2150 - lr: 0.0100\n",
            "Epoch 2/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3139\n",
            "Epoch 2: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3040 - val_loss: 0.2103 - lr: 0.0100\n",
            "Epoch 3/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2999\n",
            "Epoch 3: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3006 - val_loss: 0.2106 - lr: 0.0100\n",
            "Epoch 4/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3017\n",
            "Epoch 4: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2995 - val_loss: 0.2113 - lr: 0.0100\n",
            "Epoch 5/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2912\n",
            "Epoch 5: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2958 - val_loss: 0.2095 - lr: 0.0100\n",
            "Epoch 6/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2829\n",
            "Epoch 6: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2865 - val_loss: 0.2085 - lr: 0.0100\n",
            "Epoch 7/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2974\n",
            "Epoch 7: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2938 - val_loss: 0.2076 - lr: 0.0100\n",
            "Epoch 8/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2763\n",
            "Epoch 8: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2858 - val_loss: 0.2102 - lr: 0.0100\n",
            "Epoch 9/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2832\n",
            "Epoch 9: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2844 - val_loss: 0.2031 - lr: 0.0100\n",
            "Epoch 10/10\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2804\n",
            "Epoch 10: val_loss did not improve from 0.19880\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2826 - val_loss: 0.2042 - lr: 0.0100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "predictions = np.vstack([predictions[i] for i in range(26)])\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
        "print('rmse', rmse)\n",
        "\n",
        "#profit test\n",
        "test_period = time[n_past+train_point:]\n",
        "\n",
        "#calculate the top 10 stock we picked\n",
        "stock_selection_index = np.argsort(predictions)[:,-30:]\n",
        "stock_selection = []\n",
        "for index in stock_selection_index:\n",
        "  stock_list = []\n",
        "  for stock_i in index:\n",
        "    stock_list.append(SP500_ticker[stock_i])\n",
        "  stock_selection.append(stock_list)\n",
        "\n",
        "#trading simulate\n",
        "original_money = 0\n",
        "trading = 0\n",
        "for stock in stock_selection[0]:\n",
        "  original_money = original_money + stock_data[stock][\"Close\"][test_period[0]]\n",
        "  #print(stock_data[stock][\"Close\"][test_period[0]])\n",
        "\n",
        "\n",
        "trading = - original_money\n",
        "i = 0\n",
        "\n",
        "for stock_list in stock_selection:\n",
        "  if i==0 or 25:\n",
        "    i = i + 1\n",
        "    continue\n",
        "\n",
        "  for stock in stock_list:\n",
        "    trading = trading + stock_data[stock][\"Close\"][test_period[i-1]] - stock_data[stock][\"Close\"][test_period[i]]\n",
        "    #print(trading)\n",
        "  \n",
        "  i = i + 1\n",
        "\n",
        "end_money = 0\n",
        "\n",
        "for stock in stock_selection[-1]:\n",
        "  trading = trading + stock_data[stock][\"Close\"][test_period[-1]]\n",
        "  end_money = end_money + stock_data[stock][\"Close\"][test_period[-1]]\n",
        "  #print(stock_data[stock][\"Close\"][test_period[-1]])\n",
        "\n",
        "print('The money earned by LSTM with Fundamental and Technical data is ',trading)\n",
        "print('The money earned by LSTM with Fundamental and Technical data is %',trading*100/original_money)\n",
        "print('The money earned by LSTM with Fundamental and Technical data is %(unchange)',(end_money-original_money)*100/original_money)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udqHYWXYWMWK",
        "outputId": "44c19204-62f6-4a48-de1b-5f9789cfc0e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse 0.0024281933089145102\n",
            "The money earned by LSTM with Fundamental and Technical data is  -287.01530742645264\n",
            "The money earned by LSTM with Fundamental and Technical data is % -6.645764532413249\n",
            "The money earned by LSTM with Fundamental and Technical data is %(unchange) -6.645764532413249\n"
          ]
        }
      ]
    }
  ]
}